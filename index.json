[{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Tuan Kha\nPhone Number: 0835173787\nEmail: khantse183212@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh campus\nMajor: Software Engineer\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and fundamental services (Cloud Fundamentals, IAM, Budget, Support).\nWeek 2: Learning basic VPC and foundational networking concepts.\nWeek 3: Advanced EC2 inside VPC, NAT Gateway, Security Group, DNS Resolver.\nWeek 4: VPC Peering, Transit Gateway, and complex VPC-to-VPC connectivity.\nWeek 5: Compute Services: EC2, Auto Scaling, Backup, and Storage Gateway.\nWeek 6: Advanced Storage: S3, Glacier, FSx, and Storage Gateway.\nWeek 7: Advanced IAM, AWS Organizations, Identity Center, KMS.\nWeek 8: Database Services, ETL (Kinesis/Glue/Athena), DMS Migration.\nWeek 9: Workshop – Designing system architecture on AWS (Architecture Design).\nWeek 10: Workshop – Building Database + Backend + Frontend.\nWeek 11: Workshop – Completing Frontend + Deploying the entire system.\nWeek 12: Workshop – Testing, optimizing \u0026amp; writing the final project report.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand AWS account structure and responsibilities. Learn how to create and secure an AWS account. Set up IAM users, groups, and permission policies. Enable MFA and configure billing alerts. Become familiar with AWS Console navigation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Overview of AWS accounts and root user responsibilities - Understand IAM concepts (Users, Groups, Policies) 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ 2 - Create AWS account - Set up payment method - Verify email \u0026amp; phone number - First login and explore AWS Console 09/10/2025 09/10/2025 https://000001.awsstudygroup.com/ 3 - Secure root account + Enable MFA + Configure password policy + Limit root user usage - Set up Billing preferences and Free Tier usage monitoring 09/11/2025 09/11/2025 https://000001.awsstudygroup.com/ 4 - Create IAM Group (Administrators) - Attach AdministratorAccess policy - Create IAM User - Configure user login and enable MFA 09/12/2025 09/12/2025 https://000001.awsstudygroup.com/ 5 - Create Budget and Billing Alerts - Review account security checklist - Practice IAM login - Explore AWS Console navigation - Summarize learnings and issues encountered 09/13/2025 09/13/2025 https://000001.awsstudygroup.com/ Week 1 Achievements: Understood AWS account components:\nRoot User IAM Users IAM Groups Permission Policies Successfully created and activated a new AWS account.\nSecured the root account by enabling MFA and establishing password standards.\nSet up billing preferences including Free Tier usage alerts and budget notifications.\nCreated IAM Group and IAM User following AWS security best practices.\nEnabled MFA for IAM User and practiced secure login flow.\nLearned AWS Console navigation and how to locate services quickly.\nCompleted initial security configuration required before using AWS services in later lessons.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.1-introduction/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Traditional chatbot systems struggle without the ability to access specific information from internal documents, leading to inaccurate or irrelevant responses. This workshop solves this problem by building an architecture capable of:\nAutomation: Automatically process and index PDF documents Content Inquiry: Receive queries and guide users to relevant content Document Retrieval: Answer complex questions with accurate source citations from documents Solution Architecture The system is designed following the RAG (Retrieval-Augmented Generation) model combined with AWS Serverless to ensure scalability:\nFrontend Interface: Users interact via React Web Application\nAmazon API Gateway receives requests from Frontend AWS Amplify hosting with CloudFront CDN Amazon Cognito handles authentication Request Handling:\nApplication Load Balancer routes traffic to EC2 FastAPI Backend processes REST API requests Amazon SQS (FIFO) ensures document processing order Backend Processing:\nChatHandler: Manages conversations, saves sessions to Amazon DynamoDB RAG Service: Orchestrates vector search and LLM generation Qdrant Vector Database: Self-hosted on EC2 for vector search AI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3.5 Sonnet (LLM) and Cohere Embed Multilingual v3 (Embeddings) Amazon Textract: OCR and text extraction from PDF Amazon S3: Document storage Amazon DynamoDB: Metadata and chat history Admin Dashboard:\nReact-based interface hosted on AWS Amplify Upload and manage documents Monitor processing status View chat history Architecture Key Technologies In this workshop, you will work with the following key AWS services:\nAmazon Bedrock: The heart of AI, providing Foundation Models (Claude, Cohere) for language processing and embedding generation Amazon Textract: Build IDP pipeline to extract text from PDF documents Amazon EC2 \u0026amp; VPC: Compute and network infrastructure for backend services Amazon S3: Document and static asset storage Amazon DynamoDB: Store metadata, chat history, and document status Amazon Cognito: Authentication and user management AWS Amplify: Frontend application hosting with integrated CI/CD Amazon SQS: Message queue for document processing pipeline Qdrant (Self-hosted): Vector database for semantic search Terraform (IaC): Deploy entire infrastructure as code "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Academic Research Chatbot AWS RAG-based solution for smart academic support and research 1. Executive Summary Academic Research Chatbot is an AI assistant supporting academic research, helping students and lecturers search, summarize, and analyze scientific documents (PDFs, papers) through natural conversation with accurate source citations.\nKey Highlights:\nCore Technology: Combines IDP (Amazon Textract) to process documents (including scans) and RAG (Amazon Bedrock - Claude 3.5 Sonnet) to generate intelligent responses. Optimized Architecture: Hybrid model using 1 EC2 t3.small combined with Serverless services (Amplify, Cognito, S3, DynamoDB) to balance performance and cost. Feasibility: Serves ~50 internal users with operating costs ~$60/month, fast deployment time (20 days), and maximizes AWS Free Tier. 2. Problem Statement Current Problem Students and researchers have to work with a large number of academic documents (conference papers, journals, theses, technical reports). Many documents are old scanned PDFs (pre-2000), without a text layer, making searching for content, data, and tables very time-consuming. Public AI tools (ChatGPT, Perplexity, NotebookLM, etc.) are not directly connected to the school/department\u0026rsquo;s internal document repository, making it difficult to ensure security and access rights by subject or research group. The current infrastructure lacks a unified access point to:\nManage research documents by subject/topic. Allow researchers to ask questions directly on their own papers. Ensure answers have clear citations (paper, page, table, section). Consequence: Researchers have to manually read, take notes, and copy data from multiple papers; lecturers find it difficult to quickly synthesize information when preparing lectures or topics; academic data is scattered across many personal machines, difficult to standardize and reuse. Solution Academic Research Chatbot proposes building an internal academic Q\u0026amp;A platform based on AWS, where:\nDev/Admin loads research document repository: Upload PDFs to Amazon S3, metadata is stored in Amazon DynamoDB. An EC2 worker consumes the Amazon SQS queue, calls Amazon Textract to OCR, extract text, tables, forms, including scanned documents. Worker normalizes/chunks content, sends to Amazon Bedrock Titan Text Embeddings v2 to generate embeddings, and indexes into Qdrant on EC2. Researchers ask questions via web interface (Amplify + CloudFront): Questions are embedded, querying Qdrant to retrieve the most relevant segments (Retrieval). These segments are passed to Claude 3.5 Sonnet on Amazon Bedrock to generate answers with accurate citations (paper, page, section, table) and explanations in academic context. All access is protected by Amazon Cognito (researcher vs admin authorization), logs \u0026amp; metrics are monitored via Amazon CloudWatch + SNS (alerts on worker errors, queue backlog, high EC2 CPU). Benefits and ROI Academic Efficiency:\nReduces 40–60% of time researchers spend finding data, F1-scores, p-values, sample sizes, experimental equipment, or method descriptions from multiple papers. Reduces citation errors due to forgetting pages/tables, as the chatbot always returns sources and locations. Internal Knowledge Management: Research documents are centralized in an S3 + DynamoDB repository, easy to backup, authorize, and expand. Can be reused for many courses, topics, and labs without building a new system. Low \u0026amp; Controllable Infrastructure Costs: Hybrid model 1 EC2 + managed AI services keeps operating costs for 50 internal users at around \u0026lt; $50/month, mainly paying for EC2, 2–3 VPC endpoint interfaces, and Bedrock/Textract usage. The system is designed to be deployed in about 20 days by a team of 4, suitable for a research/internship project but still has product architecture quality. Long-term Value: Creates a platform to later integrate learning behavior analysis dashboards, paper recommendation modules, or expand to multi-language and multi-field learning assistants. 3. Solution Architecture Academic Research Chatbot applies the AWS Hybrid RAG Architecture model with IDP (Intelligent Document Processing), combining a single EC2 (FastAPI + Qdrant + Worker) with managed AI services (Textract, Bedrock) to optimize costs while ensuring performance for about 50 internal users.\nData Processing and Conversation Flow AWS Services Used\nAmazon Route 53: DNS management for chatbot platform domain. Amazon CloudFront: CDN distributing web interface (chat + admin) with low latency. AWS Amplify Hosting: Hosts web application (React/Next) for Researchers and Dev/Admin. Amazon Cognito: User authentication, researcher vs admin role management. Amazon S3: Stores original PDF files uploaded by Dev/Admin (raw documents). Amazon SQS (doc_ingestion_queue): Job queue for document processing. Amazon Textract: IDP/OCR for scanned PDFs and digital PDFs. Amazon Bedrock: Titan Text Embeddings v2: Generates embedding vectors for text chunks. Claude 3.5 Sonnet: Generates academic answers from context + user questions (RAG). Amazon DynamoDB: Documents table: document metadata, pipeline status (UPLOADED, IDP_RUNNING, EMBEDDING_DONE, FAILED). Amazon EC2 (t3.small, private subnet): Runs FastAPI backend (REST API for chat and admin). Runs Qdrant Vector DB to store and query embeddings. Runs Worker process consuming SQS, calling Textract + Titan, indexing into Qdrant, updating DynamoDB. VPC + ALB + VPC Endpoints: VPC + private subnet for EC2 (not directly exposed to Internet). Application Load Balancer (ALB): entry point for all APIs from Amplify to EC2. Gateway Endpoint (S3, DynamoDB) and Interface Endpoint (Textract, Bedrock, SQS – if used) for EC2 to call AWS services without NAT Gateway. Amazon CloudWatch + Amazon SNS: Collects logs and metrics from EC2, ALB, SQS. CloudWatch Alarms send alerts via SNS when CPU is high, SQS backlog, worker errors, etc. AWS CodePipeline / CodeBuild: Automates build \u0026amp; deploy for backend (FastAPI on EC2). Component Design\nUsers: Researchers: Q\u0026amp;A, academic content lookup. Dev/Admin: Upload, manage, and re-index documents. Document Processing (IDP): PDFs uploaded by Dev/Admin to S3. Worker on EC2 calls Textract for OCR and text/table extraction. Indexing \u0026amp; Vector DB: Worker normalizes, chunks content. Calls Bedrock Titan Embeddings v2 to create embeddings. Saves embeddings + metadata to Qdrant on EC2. AI Conversation (RAG): FastAPI embeds question, queries Qdrant for top-k relevant segments. Sends context + question to Claude 3.5 Sonnet (Bedrock) to generate answer with citation. User Management: Cognito authenticates and authorizes researcher / admin. Storage \u0026amp; State: DynamoDB stores document metadata (doc_id, status, owner, …) and (optional) chat history. 4. Technical Implementation Implementation Phases The project consists of 2 main parts — web platform (UI + auth) and RAG + IDP backend — deployed across 4 phases:\nResearch \u0026amp; Architecture Finalization: Review requirements (50 researchers, 1 EC2, IDP + RAG). Finalize architecture: VPC, EC2 (FastAPI + Qdrant + Worker), Amplify, Cognito, S3, SQS, DynamoDB, Textract, Bedrock. POC \u0026amp; Connectivity Check: Create EC2, VPC endpoints, test calling Textract, Titan Embeddings, Claude 3.5 Sonnet. Run simple Qdrant on EC2, test vector insert/search. Create skeleton FastAPI + a minimal Chat UI on Amplify. Feature Completion: Build /api/chat (FastAPI) + RAG pipeline: embed query → Qdrant → Claude + citation. Build /api/admin/: upload PDF, save to S3 + DynamoDB, push message to SQS. Write Worker on EC2: SQS → Textract → normalize/chunk → Titan → Qdrant → update DynamoDB. Complete Chat UI and Admin UI (upload + view document status). Testing, Optimization, Internal Demo Deployment: End-to-end test with a set of ~50–100 papers. Add CloudWatch Logs/Alarms, SNS notify on error or queue backlog. Adjust EC2, Qdrant configuration, batch size to optimize time and cost. Prepare user guide and demo for the group of 50 researchers. Technical Requirements\nFrontend \u0026amp; Auth: React/Next.js hosted on AWS Amplify, CloudFront CDN, Route 53 DNS. Amazon Cognito manages identity and permissions (Researcher/Admin). Backend \u0026amp; Compute: EC2 t3.small (Private Subnet) running All-in-one: FastAPI, Qdrant Vector DB, and Worker. Asynchronous processing: Worker reads SQS, triggers Textract and Bedrock to index data. IDP \u0026amp; RAG: Storage: S3 (Original files), DynamoDB (Metadata \u0026amp; Status). AI Core: Textract (OCR scanned docs), Bedrock Titan (Embedding), Claude 3.5 Sonnet (Question Answering). Network \u0026amp; Observability: Network: VPC Private Subnet, VPC Endpoints for secure connection to AWS Services. Monitoring: CloudWatch Logs/Metrics + SNS alerts (High CPU, Worker errors). 5. Timeline \u0026amp; Milestones The project is executed over approximately 6 weeks with specific phases:\nWeek 1-2 (Days 1-10): Research \u0026amp; Design Detailed architecture design, scope definition, service selection. Planning for operational cost optimization and deployment. Week 3 (Days 11-15): AWS Infrastructure Setup Configure VPC, Subnets, Security Groups, IAM Roles. Deploy EC2 t3.small, S3 bucket, DynamoDB tables. Setup VPC Endpoints (Gateway + Interface). Week 4 (Days 16-20): Backend APIs \u0026amp; IDP Pipeline Build FastAPI endpoints (/api/chat, /api/admin/upload). Integrate IDP pipeline: SQS → Worker → Textract → Embeddings → Qdrant. Connect Bedrock (Titan Embeddings + Claude 3.5 Sonnet). Week 5 (Days 21-25): Testing \u0026amp; Error Handling End-to-end testing with a set of ~50-100 papers. Handle edge cases, retry logic, error handling. Optimize chunking strategy and retrieval accuracy. Week 6 (Days 26-30): Deployment \u0026amp; Documentation Finalize UI/UX for Admin and Researcher. Setup CloudWatch Alarms + SNS notifications. Prepare user guide and demo for the group of 50 researchers. 6. Budget Estimation You can view costs on the AWS Pricing Calculator Or download the Budget Estimation File.\nInfrastructure Costs (Estimated Monthly)\nFixed Infrastructure (~$40–45): Compute \u0026amp; Network: EC2 t3.small ($15) + VPC Endpoints ($20). Storage \u0026amp; Web: S3, DynamoDB, SQS, Amplify, CloudWatch (~$5–10). AI Costs (Variable): Amazon Textract: ~$15–25 (batch processing first 10,000 pages). Amazon Bedrock: $5–15 (serving 50 users). Total: **$50–60/month** for internal research environment. 7. Risk Assessment Risk Matrix\nHallucination (AI fabrication): High impact, medium probability. Budget Overrun (AI Services): Medium impact, medium probability. Infrastructure Failure (EC2/Qdrant): High impact, low probability. Mitigation Strategies\nAI Quality: Mandatory source citations, limit input context from Qdrant. Cost: Set up AWS Budgets/Alarms, control document ingestion volume. Infrastructure \u0026amp; Security: Periodic EBS backups, data encryption (S3/DynamoDB), strict permissions via Cognito/IAM. Contingency Plans\nSystem Failure: Restore from Snapshot, pause ingestion (buffer via SQS). Cost Overrun: Temporarily lock new uploads, limit daily query quotas. 8. Expected Outcomes Technical Improvements\nTransform scattered document repositories (PDF/Scan) into digital knowledge queryable and automatically citable. Significantly reduce manual search time thanks to RAG + IDP technology. Long-term Value\nBuild a digitized research platform for 50+ researchers, easily scalable. Create a foundation for advanced features: Document recommendations, research trend analysis, and Literature Review support. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS Budgets and how to use them to manage and monitor AWS costs. Learn different types of AWS Budgets: Cost Budget, Usage Budget, RI Budget, Savings Plans Budget. Practice creating budgets using templates and custom settings, and cleaning up resources. Understand AWS Support: support plans, how to access AWS Support, and how to create and manage support requests. Build awareness of cost control and how to get help from AWS when issues occur. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study AWS Budgets overview + What is AWS Budgets? + Why use budgets to control costs? + Types of budgets (Cost, Usage, RI, Savings Plans) 09/16/2025 09/16/2025 https://000007.awsstudygroup.com/ 2 - Hands-on with Budgets (1): + Create Budget by Template + Create a Cost Budget with alert thresholds + Review budget details and notification settings 09/17/2025 09/17/2025 https://000007.awsstudygroup.com/ 3 - Hands-on with Budgets (2): + Create a Usage Budget for a specific service (e.g., EC2) + Create RI Budget + Understand when to use each type of budget 09/18/2025 09/18/2025 https://000007.awsstudygroup.com/ 4 - Hands-on with Budgets (3): + Create Savings Plans Budget + Review alerts and examples of cost overrun scenarios + Clean up budgets and related resources after the lab 09/19/2025 09/19/2025 https://000007.awsstudygroup.com/ 5 - Study AWS Support: + AWS Support Plans and their differences + How to access AWS Support Center - Hands-on: + Navigate to Support Center + Create a support case + View / update / close support requests 09/20/2025 09/20/2025 https://000009.awsstudygroup.com/ Week 2 Achievements: Understood what AWS Budgets is and why it is important for cost management. Learned the differences between Cost Budget, Usage Budget, RI Budget, and Savings Plans Budget. Successfully created budgets using templates and custom settings in the AWS console. Configured budget alerts to receive notifications when costs or usage exceed thresholds. Practiced cleaning up budgets and related resources after completing the lab. Understood the purpose of AWS Support and the different support plans available. Learned how to access AWS Support Center, create a support case, and manage existing support requests. Increased awareness of both cost control and support processes when operating workloads on AWS. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.2-preparation/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Requirements needed to complete this workshop:\nAWS Client Machine: Configured with access to necessary AWS services Development Environment: Windows, macOS, or Linux with basic development tools Basic Knowledge: Understanding of AWS, Python, JavaScript, and Docker GitHub Account: To clone source code and track changes AWS Budget: Approximately $65/month for resources (EC2, Bedrock, NAT Gateway) Tool Installation 1. AWS CLI AWS Command Line Interface (AWS CLI) is a tool to interact with AWS services.\nWindows:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\nbrew install awscli Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Verify installation:\naws --version # aws-cli/2.x.x Python/3.x.x 2. Terraform Terraform is an Infrastructure as Code tool to provision AWS resources.\nWindows:\nchoco install terraform macOS:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Linux:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Verify:\nterraform --version 3. Docker Docker to run Qdrant vector database locally and on EC2.\nWindows/macOS: Download Docker Desktop\nLinux:\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker $USER Verify:\ndocker --version docker run hello-world 4. Node.js (\u0026gt;= 18) Node.js for frontend development with React + Vite.\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js 18 nvm install 18 nvm use 18 Verify:\nnode --version # v18.x.x npm --version # 9.x.x 5. Python (\u0026gt;= 3.11) Python for backend FastAPI application.\nWindows: Download from python.org (select \u0026ldquo;Add to PATH\u0026rdquo;)\nmacOS:\nbrew install python@3.11 Linux:\nsudo apt update sudo apt install python3.11 python3.11-venv python3-pip Verify:\npython --version # Python 3.11.x pip --version 6. Git Git for version control.\nWindows: Download from git-scm.com\nmacOS:\nbrew install git Linux:\nsudo apt install git Verify:\ngit --version Clone Repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Configure AWS Credentials Create IAM User Login to AWS Console Navigate to IAM → Users → Create user User name: arc-workshop-user Attach policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Create access key → Download credentials Configure AWS CLI aws configure Enter information:\nAWS Access Key ID: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name: ap-southeast-1 Default output format: json Verify:\naws sts get-caller-identity Activate Amazon Bedrock Models Request Model Access AWS Console → Amazon Bedrock → Model access Click Manage model access Select models: Anthropic - Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20241022-v2:0) Cohere - Embed Multilingual v3 (cohere.embed-multilingual-v3) Click Request model access → Accept Terms → Submit Verify Access # Test Claude aws bedrock get-foundation-model \\ --model-identifier anthropic.claude-3-5-sonnet-20241022-v2:0 \\ --region ap-southeast-1 # Test Cohere aws bedrock get-foundation-model \\ --model-identifier cohere.embed-multilingual-v3 \\ --region ap-southeast-1 Expected: Status Access granted\nPrepare Sample Documents Project comes with sample PDFs in samples/:\nls samples/ # data-structures-sample.pdf # test-sample.pdf Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Checklist Before proceeding, ensure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with proper permissions Bedrock models approved (Claude + Cohere) Sample documents ready "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Nâng cao hiệu quả đào tạo phân tích bộ gen vi khuẩn với Amazon WorkSpaces Tác giả: Satsawat Natakarnkitkul, Charlie Lee, và Sikharin Kongpaiboon\nNgày đăng: ngày 04 tháng 04 năm 2025|\nDanh mục: Amazon WorkSpaces, Education, Healthcare, Higher education, Public Sector | Permalink|\nCác hội thảo phân tích bộ gen vi khuẩn yêu cầu các công cụ bioinformatics chuyên biệt và sức mạnh tính toán lớn để xử lý dữ liệu giải trình tự. Khi Trung tâm Nghiên cứu Y khoa Siriraj lên kế hoạch tổ chức “Hội thảo Nanopore: chuỗi hội thảo tin sinh học về bộ gen vi khuẩn” cho hơn 60 nhà nghiên cứu, họ đã gặp phải thách thức phổ biến: làm thế nào để cung cấp môi trường tính toán hiệu suất cao, đồng nhất cho các phân tích bộ gen phức tạp. Amazon Web Services (AWS) đã giải quyết vấn đề này thông qua Amazon WorkSpaces, thay đổi cách Trung tâm Nghiên cứu Y khoa Siriraj cung cấp đào tạo thực hành về bộ gen vi khuẩn.\nBạn có thể sử dụng Amazon WorkSpaces để cấp phát các máy tính để bàn ảo trên nền tảng đám mây, gọi là WorkSpaces, cho người dùng của mình. Những máy tính để bàn này có thể chạy Microsoft Windows, Amazon Linux 2, Ubuntu Linux, Rocky Linux hoặc Red Hat Enterprise Linux. WorkSpaces loại bỏ nhu cầu mua sắm và triển khai phần cứng hoặc cài đặt phần mềm phức tạp. Bạn có thể nhanh chóng thêm hoặc bớt người dùng khi nhu cầu thay đổi. Người dùng có thể truy cập máy tính để bàn ảo của mình từ nhiều thiết bị hoặc trình duyệt web khác nhau. Với những lợi ích này, người dùng có thể làm việc hiệu quả mà không cần phải lo lắng về máy tính để bàn của họ.\nThách thức Các hội thảo đào tạo về bộ gen vi khuẩn do Trung tâm Nghiên cứu Y khoa Siriraj tổ chức thường gặp phải nhiều khó khăn tài nguyên tính toán, cài đặt phần mềm, cũng như khả năng tiếp cận và mở rộng cơ sở hạ tầng.\nMột trong những thách thức lớn nhất là sự khác biệt về cấu hình phần cứng giữa các học viên. Nhiều người dùng phải sử dụng laptop cá nhân với hệ điều hành, tốc độ xử lý, bộ nhớ khác nhau, dẫn đến hiệu suất không đồng đều. Sự khác biệt này thường gây ra lỗi hệ thống, tốc độ xử lý chậm và không thể chạy các phân tích genome đòi hỏi tài nguyên lớn hiệu quả.\nViệc cài đặt và cấu hình phần mềm là trở ngại chính. Các công cụ bioinformatics như EPI2ME, phổ biến trong việc lắp ráp và phân tích bộ gen vi khuẩn, yêu cầu các gói phụ thuộc (dependencies) và cấu hình đặc thù. Đảm bảo tương thích trên nhiều thiết bị khác nhau thường làm gián đoạn tiến độ, khiến giảng viên mất thời gian xử lý sự cố thay vì tập trung vào thực hành.\nXử lý dữ liệu bộ gen vi khuẩn cần máy tính mạnh mà phần đông học viên không có sẵn. Thiếu tài nguyên tính toán dẫn đến việc không hoàn thành bài tập, gây thất vọng và làm giảm hiệu quả học tập. Laptop cá nhân thường không đủ khả năng xử lý các phép tính phức tạp trong phân tích dữ liệu DNA, làm hạn chế việc thực hành.\nPhương pháp giảng dạy truyền thống cũng giới hạn số lượng học viên mà tổ chức có thể đào tạo. Nhiều tổ chức thiếu cơ sở hạ tầng cần thiết để đáp ứng nhu cầu đào tạo ngày càng tăng, đặc biệt cho các buổi đào tạo kết hợp hoặc trực tuyến.\nXây dựng môi trường học tập trên nền tảng đám mây Oxford Nanopore Centre of Excellence (Thái Lan), Yip In Tsoi ( Đối tác AWS) và AWS đã phối hợp khởi xướng và tổ chức một hội thảo nhằm giải quyết những thách thức cấp bách này trong đào tạo tin sinh học, đặc biệt liên quan đến giới hạn phần cứng, vấn đề tương thích phần mềm, và sức mạnh tính toán. Hội thảo hợp tác này sử dụng WorkSpaces như một môi trường tính toán tập trung trên đám mây cho tất cả học viên, cung cấp một môi trường học tập tiêu chuẩn trên đám mây. WorkSpaces là dịch vụ máy tính để bàn ảo được quản lý toàn phần, cung cấp tài nguyên tính toán đã được cấu hình sẵn, giúp đảm bảo mọi học viên có cùng môi trường làm việc.\nMột trong những lợi ích chính của WorkSpaces là loại bỏ sự trì hoãn do cài đặt và cấu hình phần mềm. Tất cả các công cụ bioinformatics cần thiết cho hội thảo—bao gồm EPI2ME—đã được cài đặt sẵn và tối ưu, giúp học viên bắt đầu thực hành ngay lập tức.\nSức mạnh tính toán của WorkSpaces đóng vai trò quan trọng nâng cao trải nghiệm hội thảo. Nhờ truy cập nguồn tài nguyên đám mây mạnh mẽ, học viên có thể thực hiện lắp ráp bộ gen, căn chỉnh trình tự, và phân tích so sánh genome mà không gặp giới hạn về hiệu suất. Sự đồng đều trong tài nguyên tính toán giúp mọi học viên hoàn thành bài tập cùng tiến độ, tạo môi trường học tập hiệu quả hơn.\nKhả năng mở rộng và tiếp cận cũng là điểm mạnh của WorkSpaces. Học viên có thể đăng nhập WorkSpaces từ bất kỳ thiết bị nào, loại bỏ giới hạn về địa lý và phần cứng. Tính linh hoạt của WorkSpaces cũng giúp tổ chức dễ dàng đáp ứng số lượng học viên lớn mà không phải lo lắng về hạ tầng tính toán.\nKết quả hội thảo Trong ba ngày hội thảo, WorkSpaces được sử dụng rộng rãi. Học viên tham gia các bài tập như giải trình tự bằng công nghệ Oxford Nanopore (ONT), lắp ráp bộ gen bằng EPI2ME, và phân tích so sánh bộ gen gồm cgMLST và nghiên cứu ổ dịch.\nViệc triển khai WorkSpaces giúp hội thảo diễn ra suôn sẻ mà không gặp khó khăn kỹ thuật, cải thiện đáng kể so với các lần trước khi việc cài đặt và khắc phục lỗi có thể mất đến 3 giờ đồng hồ. Hiệu suất tính toán được giữ ổn định, giúp học viên tập trung phân tích dữ liệu thay vì gặp sự cố hoặc xử lý chậm.\nPhản hồi từ học viên cho thấy WorkSpaces đã giúp nâng cao hiệu quả đào tạo tin sinh học. Nhiều học viên cho biết môi trường tính toán tiêu chuẩn giúp họ tập trung vào nội dung học mà không phải lo ngại về hạn chế phần cứng hay lỗi phần mềm. Giảng viên cũng nhận thấy hiệu quả giảng dạy được cải thiện rõ rệt khi không phải mất thời gian xử lý các vấn đề kỹ thuật.\nTương lai phát triển Việc tích hợp thành công WorkSpaces với hội thảo Nanopore cho thấy công nghệ đám mây có thể thay đổi cách đào tạo. Bởi vì các rào cản công nghệ truyền thống đã được loại bỏ, học viên có thể tập trung hơn vào phân tích bộ gen vi khuẩn.\nTrong tương lai, có tiềm năng mở rộng sử dụng WorkSpaces trong các chương trình đào tạo genomics khác—đặc biệt là các dự án quy mô lớn—như:\nNâng cao khả năng phân tích bộ gen với AWS HealthOmics Xử lý và phân tích bộ dữ liệu lớn với AWS Batch Tự động hóa quy trình phân tích genome với AWS Lambda Những giải pháp này có thể cải thiện hơn nữa giáo dục tin sinh học bằng cách tối ưu hóa quy trình phân tích dữ liệu. Các tổ chức cũng đang nghiên cứu tích hợp WorkSpaces vào các mô hình học từ xa, giúp mở rộng sự tham gia của sinh viên và nhà nghiên cứu trên toàn cầu.\nĐể tìm hiểu thêm và bắt đầu, hãy liên hệ với nhóm tài khoản AWS của bạn hoặc nhóm hỗ trợ AWS Public Sector team.\nTài nguyên bổ sung Genomics on AWS Amazon WorkSpaces Documentation Siriraj Long-read Lab (Si-LoL), Oxford Nanopore Centre of Excellence (Thailand) Được đồng viết bởi Oxford Nanopore Centre of Excellence (Thailand), YIP In Tsoi (Đối tác AWS), và AWS.\nTAGS : Amazon Workspaces,AWS education,AWS for higher education,AWS healthcare, genomics, healthcare\nTác giả: Satsawat Natakarnkitkul: Trưởng nhóm dữ liệu và AI cho khu vực ASEAN tại AWS Thái Lan, dẫn dắt các sáng kiến AI tạo sinh trên khắp Đông Nam Á. Với hơn một thập kỷ kinh nghiệm trong chuyển đổi số và các giải pháp AI/ML, ông là chuyên gia đầu ngành trong trí tuệ nhân tạo, khoa học dữ liệu, và kiến trúc đám mây. Ông thường xuyên phát biểu tại các sự kiện công nghệ khu vực ASEAN, và nhiệt huyết sử dụng công nghệ mới như AI tạo sinh để tạo giá trị thực tế trong khu vực công.\nCharlie Lee: Chuyên gia hàng đầu về ngành genomics khu vực châu Á - Thái Bình Dương và Nhật Bản của AWS, có bằng tiến sĩ khoa học máy tính chuyên sâu về tin sinh học. Ông là nhà lãnh đạo trong lĩnh vực tin sinh học, genomics, và chẩn đoán phân tử, đam mê thúc đẩy nghiên cứu và cải thiện chăm sóc sức khỏe qua genomics với các công nghệ giải trình tự tiên tiến và điện toán đám mây.\nSikharin Kongpaiboon: Kiến trúc sư giải pháp tại AWS, hỗ trợ khách hàng hiểu và áp dụng tốt nhất các giải pháp đám mây, tối ưu triển khai. Ông phối hợp chặt chẽ với khách hàng để thiết kế kiến trúc đám mây có khả năng mở rộng, linh hoạt, và chịu lỗi cao, giúp giải quyết các thách thức kinh doanh và tăng tính nhanh nhẹn, hiệu quả, và an toàn.\nCác tài nguyên: AWS in the Public Sector\nAWS for Government\nAWS for Education\nAWS for Nonprofits\nAWS for Public Sector Health\nAWS for Aerospace and Satellite Solutions\nCase Studies\nFix This Podcast\nAdditional Resources\nContact Us\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Enhance deployment guardrails with inference component rolling updates for Amazon SageMaker AI inference Amazon SageMaker AI now supports rolling updates for inference components, solving key problems of traditional blue/green deployments—like high GPU cost, limited capacity, and risky all-in-one traffic shifts—especially for large LLMs on expensive instances. Inference components already help by right-sizing resources and scaling per model, and rolling updates extend this by updating model copies in configurable batches, temporarily adding just enough capacity for each step, shifting traffic to the new version, and then tearing down the old capacity. You control batch size, timeouts, rollback behavior, and wait intervals through the RollingUpdatePolicy in UpdateInferenceComponent, while CloudWatch alarms can trigger automatic, controlled rollbacks if issues arise. For example, with three single-GPU instances, SageMaker can update one copy at a time by launching a new instance, validating it, then removing one old copy, repeating this until all copies are updated—achieving zero downtime, safer validation, and more cost-efficient, reliable deployments for models of different sizes.\nBlog 2 - Empowering bacterial genomics education with Amazon WorkSpaces TSiriraj Medical Research Center’s “Nanopore workshop: bacterial genome bioinformatics series” needed powerful, consistent computing environments for over 60 researchers, but faced big obstacles: participants had very different laptops and operating systems, many too weak for heavy bacterial genome analysis; installing and configuring complex bioinformatics tools like EPI2ME on each personal device was slow and error-prone; and limited on-premise infrastructure made it hard to scale or support hybrid/remote training. By using Amazon WorkSpaces—virtual cloud desktops that can run various Linux or Windows systems and be accessed from many devices— they could centrally provide preconfigured, high-performance environments, avoid hardware procurement and individual setup issues, and deliver smoother, more scalable hands-on bacterial genomics training.\nBlog 3 - End of support notifications and enhanced discoverability for Amazon EKS In the rapidly evolving world of containerized applications, maintaining resilience and observability across Kubernetes environments has become a critical challenge. As organizations increasingly adopt Amazon Elastic Kubernetes Service (Amazon EKS) to manage their containerized workloads, the need for cluster version lifecycle management and discovery mechanisms becomes crucial. As Amazon EKS environments grow more complex and span multiple AWS Regions and accounts, users often struggle to track cluster versions, support lifecycles, and overall deployment status.\nProactive monitoring of EKS cluster lifecycles and end of support is crucial to making sure of the security, stability, and compliance of Kubernetes deployments. Furthermore, gaining visibility into EKS cluster deployments across an entire AWS Organization is essential for effective resource management, strategic planning, and maintaining an accurate inventory.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Tăng cường deployment guardrails với tính năng rolling update cho inference component trên Amazon SageMaker AI inference Bài viết này có đồng tác giả là : Melanie Li, Andrew Smith, Dustin Liu, Vivek Gangasani, Shikhar Mishra, và June Won\nNgày: 25 tháng 3, 2025 | in Amazon SageMaker, Amazon SageMaker AI, Intermediate (200) Permalink | Comments | Share\nTriển khai các mô hình một cách hiệu quả, tin cậy và tiết kiệm chi phí là một thách thức quan trọng đối với các tổ chức ở mọi quy mô. Khi ngày càng nhiều tổ chức triển khai các foundation model (FM) và các mô hình machine learning (ML) khác vào môi trường production, họ phải đối mặt với những thách thức liên quan đến việc tận dụng tài nguyên, hiệu quả chi phí và duy trì tính khả dụng cao trong quá trình cập nhật. Amazon SageMaker AI đã giới thiệu chức năng inference component có thể giúp các tổ chức giảm chi phí triển khai mô hình bằng cách tối ưu hóa việc sử dụng tài nguyên thông qua khả năng đóng gói và mở rộng mô hình thông minh. Inference component trừu tượng hóa các mô hình ML và cho phép phân bổ tài nguyên chuyên dụng cũng như các chính sách mở rộng cụ thể cho từng mô hình.\nTuy nhiên, việc cập nhật các mô hình này—đặc biệt trong môi trường production với các SLA về độ trễ nghiêm ngặt—về mặt lịch sử có nguy cơ gây downtime hoặc tắc nghẽn tài nguyên. Các triển khai blue/green truyền thống thường gặp khó khăn với những hạn chế về năng lực, khiến việc cập nhật trở nên khó dự đoán đối với các mô hình sử dụng GPU nặng. Để giải quyết vấn đề này, chúng tôi rất vui mừng công bố một cải tiến mạnh mẽ khác cho RageMaker AI:tính năng rolling update cho inference component endpoint, một tính năng được thiết kế để đơn giản hóa việc cập nhật cho các mô hình có kích thước khác nhau đồng thời giảm thiểu chi phí vận hành.\nTrong bài viết này, chúng tôi thảo luận về những thách thức mà các tổ chức phải đối mặt khi cập nhật mô hình trong production. Sau đó, chúng tôi đi sâu vào tính năng rolling update mới cho inference component và cung cấp các ví dụ thực tế sử dụng các mô hình DeepSeek distilled để minh họa tính năng này. Cuối cùng, chúng tôi khám phá cách thiết lập rolling update trong các tình huống khác nhau.\nNhững thách thức với triển khai blue/green Theo truyền thống, RageMaker AI inference đã hỗ trợ mô hình triển khai blue/green để cập nhật inference component trong production. Mặc dù có hiệu quả trong nhiều tình huống, cách tiếp cận này đi kèm với những thách thức cụ thể:\nKém hiệu quả về tài nguyên – Triển khai blue/green yêu cầu cung cấp tài nguyên cho cả môi trường hiện tại (blue) và môi trường mới (green) đồng thời. Đối với các inference component chạy trên các GPU instance đắt tiền như P4d hoặc G5, điều này có nghĩa là có khả năng tăng gấp đôi yêu cầu tài nguyên trong quá trình triển khai. Hãy xem xét một ví dụ trong đó khách hàng có 10 bản sao của một inference component phân bố trên 5 instance ml.p4d.24xlarge, tất cả đều hoạt động với công suất đầy đủ. Với triển khai blue/green, SageMaker AI sẽ cần cung cấp thêm năm instance ml.p4d.24xlarge để lưu trữ phiên bản mới của inference component trước khi chuyển đổi lưu lượng và ngừng sử dụng các instance cũ. Tài nguyên tính toán hạn chế – Đối với khách hàng sử dụng các GPU instance mạnh mẽ như dòng P hoặc G, năng lực cần thiết có thể không có sẵn trong một Availability Zone hoặc Region nhất định. Điều này thường dẫn đến các ngoại lệ về năng lực instance trong quá trình triển khai, gây ra lỗi cập nhật và rollback. Chuyển đổi theo kiểu tất cả hoặc không có gì – Các triển khai blue/green truyền thống chuyển toàn bộ lưu lượng cùng một lúc hoặc dựa trên lịch trình được cấu hình. Điều này để lại không gian hạn chế cho việc xác thực dần dần và tăng phạm vi ảnh hưởng nếu có vấn đề phát sinh với triển khai mới. Mặc dù triển khai blue/green đã là một chiến lược đáng tin cậy cho các bản cập nhật zero-downtime, những hạn chế của nó trở nên rõ ràng khi triển khai các large language model (LLM) quy mô lớn hoặc các mô hình thông lượng cao trên các GPU instance cao cấp. Những thách thức này đòi hỏi một cách tiếp cận tinh tế hơn—một cách tiếp cận xác thực các bản cập nhật từng bước trong khi tối ưu hóa việc sử dụng tài nguyên. Rolling update cho inference component được thiết kế để loại bỏ sự cứng nhắc của các triển khai blue/green. Bằng cách cập nhật các mô hình theo các batch được kiểm soát, mở rộng cơ sở hạ tầng một cách linh hoạt và tích hợp các kiểm tra an toàn theo thời gian thực, chiến lược này đảm bảo các triển khai vẫn tiết kiệm chi phí, đáng tin cậy và có khả năng thích ứng—ngay cả đối với các workload sử dụng GPU nặng.\nRolling deployment để cập nhật inference component Như đã đề cập trước đó, inference component được giới thiệu như một tính năng của RageMaker AI để tối ưu hóa chi phí; chúng cho phép bạn định nghĩa và triển khai các tài nguyên cụ thể cần thiết cho workload suy luận mô hình của bạn. Bằng cách điều chỉnh đúng kích thước tài nguyên tính toán để phù hợp với yêu cầu của mô hình, bạn có thể tiết kiệm chi phí trong quá trình cập nhật so với các phương pháp triển khai truyền thống.\nVới rolling update, RageMaker AI triển khai các phiên bản mô hình mới theo các batch inference component có thể cấu hình trong khi mở rộng instance một cách linh hoạt. Điều này đặc biệt có tác động đối với các LLM:\nTính linh hoạt về kích thước batch – Khi cập nhật các inference component trong một SageMaker AI endpoint, bạn có thể chỉ định kích thước batch cho mỗi bước rolling. Đối với mỗi bước, RageMaker AI cung cấp năng lực dựa trên kích thước batch được chỉ định trên endpoint fleet mới, định tuyến lưu lượng đến fleet đó và dừng năng lực trên endpoint fleet cũ. Các mô hình nhỏ hơn như DeepSeek Distilled Llama 8B có thể sử dụng các batch lớn hơn để cập nhật nhanh chóng, và các mô hình lớn hơn như DeepSeek Distilled Llama 70B sử dụng các batch nhỏ hơn để hạn chế tranh chấp GPU. Bảo vệ an toàn tự động – Các alarm Amazon CloudWatch tích hợp giám sát các metric trên một inference component. Bạn có thể cấu hình các alarm để kiểm tra xem phiên bản mới được triển khai của inference component có hoạt động đúng hay không. Nếu các alarm CloudWatch được kích hoạt, RageMaker AI sẽ bắt đầu một rollback tự động. Chức năng mới được triển khai thông qua các phần mở rộng cho RageMaker AI API, chủ yếu với các tham số mới trong API Update Inference Component:\nsagemaker_client.update_inference_component(\nInferenceComponentName=inference_component_name,\nRuntimeConfig={ \u0026ldquo;CopyCount\u0026rdquo;: number },\nSpecification={ \u0026hellip; },\nDeploymentConfig={\n\u0026ldquo;RollingUpdatePolicy\u0026rdquo;: {\n\u0026ldquo;MaximumBatchSize\u0026rdquo;: { # Value must be between 5% to 50% of the IC\u0026rsquo;s total copy count.\n\u0026ldquo;Type\u0026rdquo;: \u0026ldquo;COPY_COUNT\u0026rdquo;, # COPY_COUNT | CAPACITY_PERCENT\n\u0026ldquo;Value\u0026rdquo;: 1 # Minimum value of 1\n},\n\u0026ldquo;MaximumExecutionTimeoutInSeconds\u0026rdquo;: 600, #Minimum value of 600. Maximum value of 28800.\n\u0026ldquo;RollbackMaximumBatchSize\u0026rdquo;: {\n\u0026ldquo;Type\u0026rdquo;: \u0026ldquo;COPY_COUNT\u0026rdquo;, # COPY_COUNT | CAPACITY_PERCENT\n\u0026ldquo;Value\u0026rdquo;:1\n},\n\u0026ldquo;WaitIntervalInSeconds\u0026rdquo;: 120 # Minimum value of 0. Maximum value of 3600\n}\n},\nAutoRollbackConfiguration={\n\u0026ldquo;Alarms\u0026rdquo;: [\n{\n\u0026ldquo;AlarmName\u0026rdquo;: \u0026ldquo;string\u0026rdquo; #Optional\n}\n]\n},\n)\nĐoạn code trên sử dụng các tham số sau:\nMaximumBatchSize – Đây là một tham số bắt buộc và định nghĩa kích thước batch cho mỗi bước rolling trong quy trình triển khai. Đối với mỗi bước, RageMaker AI cung cấp năng lực trên endpoint fleet mới, định tuyến lưu lượng đến fleet đó và dừng năng lực trên endpoint fleet cũ. Giá trị phải nằm trong khoảng từ 5–50% số lượng bản sao của inference component. Type – Tham số này có thể chứa một giá trị như COPY_COUNT | CAPACITY_PERCENT, chỉ định loại năng lực endpoint. Value – Định nghĩa kích thước năng lực, dưới dạng số lượng bản sao inference component hoặc phần trăm năng lực. MaximumExecutionTimeoutSeconds – Đây là thời gian tối đa mà rolling deployment sẽ dành cho việc thực thi tổng thể. Vượt quá giới hạn này sẽ gây ra timeout. RollbackMaximumBatchSize – Đây là kích thước batch cho một rollback về endpoint fleet cũ. Nếu trường này vắng mặt, giá trị được đặt thành mặc định, là 100% tổng năng lực. Khi sử dụng mặc định, RageMaker AI cung cấp toàn bộ năng lực của fleet cũ cùng một lúc trong quá trình rollback. Value – Tham số Value của cấu trúc này sẽ chứa giá trị mà Type sẽ được thực thi. Đối với chiến lược rollback, nếu bạn không chỉ định các trường trong đối tượng này, hoặc nếu bạn đặt Value thành 100%, thì SageMaker AI sử dụng chiến lược rollback blue/green và roll lưu lượng trở lại fleet blue. WithIntervalInSeconds – Đây là giới hạn thời gian cho tổng triển khai. Vượt quá giới hạn này sẽ gây ra timeout. AutoRollbackConfiguration – Đây là cấu hình rollback tự động để xử lý lỗi triển khai endpoint và khôi phục. AlarmName – Alarm CloudWatch này được cấu hình để giám sát các metric trên một InferenceComponent Bạn có thể cấu hình nó để kiểm tra xem phiên bản mới được triển khai của InferenceComponent có hoạt động đúng hay không. Để biết thêm thông tin về SageMaker AI API, tham khảo SageMaker AI API Reference.\nTrải nghiệm khách hàng Hãy cùng khám phá cách rolling update hoạt động trong thực tế với một số tình huống phổ biến, sử dụng các LLM có kích thước khác nhau. Bạn có thể tìm thấy notebook ví dụ trong GitHub repo.\nTình huống 1: Nhiều cluster GPU đơn Trong tình huống này, giả sử bạn đang chạy một endpoint với ba instance ml.g5.2xlarge, mỗi instance có một GPU duy nhất. Endpoint lưu trữ một inference component yêu cầu một CPU accelerator, có nghĩa là mỗi instance chứa một bản sao. Khi bạn muốn cập nhật inference component để sử dụng phiên bản inference component mới, bạn có thể sử dụng rolling update để giảm thiểu sự gián đoạn.\nBạn có thể cấu hình một rolling update với kích thước batch là một, có nghĩa là RageMaker AI sẽ cập nhật từng bản sao một. Trong quá trình cập nhật, RageMaker AI trước tiên xác định năng lực có sẵn trong các instance hiện có. Vì không có instance hiện tại nào có không gian cho các workload tạm thời bổ sung, RageMaker AI sẽ khởi chạy các instance ml.g5.2xlarge mới từng cái một để triển khai một bản sao của phiên bản inference component mới lên một GPU instance. Sau khoảng thời gian chờ được chỉ định và container của inference component mới vượt qua kiểm tra healthy, RageMaker AI loại bỏ một bản sao của phiên bản cũ (vì mỗi bản sao được lưu trữ trên một instance, instance này sẽ được dỡ bỏ tương ứng), hoàn tất bản cập nhật cho batch đầu tiên.\nQuy trình này lặp lại cho bản sao thứ hai của inference component, cung cấp một quá trình chuyển đổi suôn sẻ với zero downtime. Bản chất dần dần của bản cập nhật giảm thiểu rủi ro và cho phép bạn duy trì tính khả dụng nhất quán trong suốt quy trình triển khai. Sơ đồ sau đây cho thấy quy trình này.\nTình huống 2: Cập nhật với rollback tự động Trong một tình huống khác, bạn có thể đang cập nhật inference component của mình từ Llama-3.1-8B-Instruct sang DeepSeek-R1-Distill-Llama-8B, nhưng phiên bản mô hình mới có các kỳ vọng API khác nhau. Trong trường hợp sử dụng này, bạn đã cấu hình một alarm CloudWatch để giám sát lỗi 4xx, điều này sẽ cho biết các vấn đề về tương thích API.\nBạn có thể bắt đầu một rolling update với kích thước batch là một bản sao. RageMaker AI triển khai bản sao đầu tiên của phiên bản mới trên một GPU instance mới. Khi instance mới sẵn sàng phục vụ lưu lượng, RageMaker AI sẽ chuyển tiếp một phần các yêu cầu invocation đến mô hình mới này. Tuy nhiên, trong ví dụ này, phiên bản mô hình mới, đang thiếu cấu hình biến môi trường \u0026ldquo;MESSAGES_API_ENABLED\u0026rdquo;, sẽ bắt đầu trả về lỗi 4xx khi nhận các yêu cầu ở định dạng Messages API.\nAlarm CloudWatch được cấu hình phát hiện các lỗi này và chuyển sang trạng thái alarm. RageMaker AI tự động phát hiện trạng thái alarm này và bắt đầu quy trình rollback theo cấu hình rollback. Theo kích thước batch rollback được chỉ định, RageMaker AI loại bỏ phiên bản mô hình mới có vấn đề và duy trì phiên bản gốc đang hoạt động, ngăn chặn sự gián đoạn dịch vụ trên diện rộng. Endpoint trở về trạng thái ban đầu với lưu lượng được xử lý bởi phiên bản mô hình gốc hoạt động đúng.\nĐoạn code sau đây cho thấy cách thiết lập một alarm CloudWatch để giám sát lỗi 4xx:\nCreate alarm cloudwatch.put_metric_alarm(\nAlarmName=f\u0026rsquo;SageMaker-{endpoint_name}-4xx-errors',\nComparisonOperator=\u0026lsquo;GreaterThanThreshold\u0026rsquo;,\nEvaluationPeriods=1,\nMetricName=\u0026lsquo;Invocation4XXErrors\u0026rsquo;,\nNamespace=\u0026lsquo;AWS/SageMaker\u0026rsquo;,\nPeriod=300,\nStatistic=\u0026lsquo;Sum\u0026rsquo;,\nThreshold=5.0,\nActionsEnabled=True,\nAlarmDescription=\u0026lsquo;Alarm when greather than 5 4xx errors\u0026rsquo;,\nDimensions=[\n{\n\u0026lsquo;Name\u0026rsquo;: \u0026lsquo;InferenceComponentName\u0026rsquo;,\n\u0026lsquo;Value\u0026rsquo;: inference_component_name\n},\n],\n)\nSau đó bạn có thể sử dụng alarm CloudWatch này trong yêu cầu cập nhật:\nDeploymentConfig={\n\u0026ldquo;RollingUpdatePolicy\u0026rdquo;: {\n\u0026ldquo;MaximumBatchSize\u0026rdquo;: {\n\u0026ldquo;Type\u0026rdquo;: \u0026ldquo;COPY_COUNT\u0026rdquo;,\n\u0026ldquo;Value\u0026rdquo;: 1\n},\n\u0026ldquo;WaitIntervalInSeconds\u0026rdquo;: 120,\n\u0026ldquo;RollbackMaximumBatchSize\u0026rdquo;: {\n\u0026ldquo;Type\u0026rdquo;: \u0026ldquo;COPY_COUNT\u0026rdquo;,\n\u0026ldquo;Value\u0026rdquo;: 1\n}\n},\n\u0026lsquo;AutoRollbackConfiguration\u0026rsquo;: {\n\u0026ldquo;Alarms\u0026rdquo;: [\n{\u0026ldquo;AlarmName\u0026rdquo;: f\u0026rsquo;SageMaker-{endpoint_name}-4xx-errors\u0026rsquo;}\n]\n}\n}\nTình huống 3: Cập nhật với năng lực đầy đủ trong các instance hiện có Nếu một endpoint hiện có có nhiều GPU accelerator và không phải tất cả các accelerator đều được sử dụng, bản cập nhật có thể sử dụng các GPU accelerator hiện có mà không cần khởi chạy các instance mới cho endpoint. Xem xét nếu bạn có một endpoint được cấu hình với hai instance ml.g5.12xlarge ban đầu có bốn GPU accelerator trong mỗi instance. Endpoint lưu trữ hai inference component: IC-1 yêu cầu một accelerator và IC-2 cũng yêu cầu một accelerator. Trên một instance ml.g5.12xlarge, có bốn bản sao của IC-1 đã được tạo; trên instance khác, hai bản sao của IC-2 đã được tạo. Vẫn còn hai GPU accelerator có sẵn trên instance thứ hai.\nKhi bạn bắt đầu một bản cập nhật cho IC-1 với kích thước batch là hai bản sao, RageMaker AI xác định rằng có đủ năng lực trong các instance hiện có để lưu trữ các phiên bản mới trong khi duy trì các phiên bản cũ. Nó sẽ tạo hai bản sao của phiên bản IC-1 mới trên instance thứ hai. Khi các container đã khởi động và chạy, SageMaker AI sẽ hướng lưu lượng đến các IC-1 mới và sau đó bắt đầu định tuyến lưu lượng đến các inference component mới. RageMaker AI cũng sẽ loại bỏ hai trong số các bản sao IC-1 cũ khỏi instance. Bạn không bị tính phí cho đến khi các inference component mới bắt đầu nhận các invocation và tạo ra các phản hồi.\nBây giờ có thêm hai GPU slot trống. RageMaker AI sẽ cập nhật batch thứ hai, và nó sẽ sử dụng các GPU accelerator trống vừa có sẵn. Sau khi các quy trình hoàn tất, endpoint có bốn IC-1 với phiên bản mới và hai bản sao của IC-2 không bị thay đổi.\nTình huống 4: Cập nhật yêu cầu năng lực instance bổ sung Xem xét nếu bạn có một endpoint được cấu hình với ban đầu một instance ml.g5.12xlarge (tổng cộng 4 GPU) và được cấu hình managed instance scaling (MIS) với số instance tối đa được đặt thành hai. Endpoint lưu trữ hai inference component: IC-1 yêu cầu 1 GPU với hai bản sao (Llama 8B), và IC-2 (mô hình DeepSeek Distilled Llama 14B) cũng yêu cầu 1 GPU với hai bản sao—sử dụng tất cả 4 GPU có sẵn.\nKhi bạn bắt đầu một bản cập nhật cho IC-1 với kích thước batch là hai bản sao, RageMaker AI xác định rằng không có đủ năng lực trong các instance hiện có để lưu trữ các phiên bản mới trong khi duy trì các phiên bản cũ. Thay vì làm thất bại bản cập nhật, vì bạn đã cấu hình MIS, RageMaker AI sẽ tự động cung cấp một instance g5.12.xlarge thứ hai để lưu trữ các inference component mới.\nTrong quá trình cập nhật, RageMaker AI triển khai hay bản sao của phiên bản IC-1 mới lên instance mới được cung cấp, như được hiển thị trong sơ đồ sau. Sau khi các inference component mới đã khởi động và chạy, RageMaker AI bắt đầu loại bỏ các bản sao IC-1 cũ khỏi các instance gốc. Đến cuối bản cập nhật, instance đầu tiên sẽ lưu trữ IC-2 sử dụng 2 GPU, và instance thứ hai mới được cung cấp sẽ lưu trữ IC-1 đã cập nhật với hai bản sao sử dụng 2 GPU. Sẽ có các không gian mới có sẵn trong hai instance, và bạn có thể triển khai thêm các bản sao inference component hoặc các mô hình mới vào cùng một endpoint bằng cách sử dụng các tài nguyên GPU có sẵn. Nếu bạn thiết lập managed instance auto scaling và đặt inference component auto scaling về không, bạn có thể scale down các bản sao inference component về không, điều này sẽ dẫn đến instance tương ứng được scale down. Khi inference component được scale up, SageMaker AI sẽ khởi chạy các inference component trong instance hiện có với các GPU accelerator có sẵn, như đã đề cập trong tình huống 3.\nTình huống 5: Cập nhật đối mặt với năng lực không đủ Trong các tình huống không có đủ năng lực GPU, RageMaker AI cung cấp phản hồi rõ ràng về các ràng buộc năng lực. Xem xét nếu bạn có một endpoint chạy trên 30 instance ml.g6e.16xlarge, mỗi instance đã được sử dụng đầy đủ với các inference component. Bạn muốn cập nhật một inference component hiện có bằng cách sử dụng rolling deployment với kích thước batch là 4, nhưng sau khi bốn batch đầu tiên được cập nhật, không có đủ năng lực GPU có sẵn cho phần còn lại của bản cập nhật. Trong trường hợp này, RageMaker AI sẽ tự động rollback về thiết lập trước đó và dừng quy trình cập nhật.\nCó thể có hai trường hợp cho trạng thái cuối cùng của rollback này. Trong trường hợp đầu tiên, rollback đã thành công vì có năng lực mới có sẵn để khởi chạy các instance cho phiên bản mô hình cũ. Tuy nhiên, có thể có một trường hợp khác trong đó vấn đề năng lực vẫn tồn tại trong quá trình rolling back, và endpoint sẽ hiển thị là UPDATE_ROLLBACK_FAILED. Các instance hiện có vẫn có thể phục vụ lưu lượng, nhưng để chuyển endpoint ra khỏi trạng thái failed, bạn cần liên hệ với nhóm AWS support của mình.\nCác cân nhắc bổ sung Như đã đề cập trước đó, khi sử dụng triển khai blue/green để cập nhật các inference component trên một endpoint, bạn cần cung cấp tài nguyên cho cả môi trường hiện tại (blue) và môi trường mới (green) đồng thời. Khi bạn đang sử dụng rolling update cho các inference component trên endpoint, bạn có thể sử dụng phương trình sau để tính số lượng service quota tài khoản cho loại instance cần thiết. GPU instance cần thiết cho endpoint có X số GPU accelerator, và mỗi bản sao inference component yêu cầu Y số GPU accelerator. Kích thước batch tối đa được đặt thành Z và endpoint hiện tại có N instance. Do đó, service quota cấp tài khoản cần thiết cho loại instance này cho endpoint phải lớn hơn đầu ra của phương trình:\nROUNDUP(Z x Y / X) + N\nVí dụ, giả sử endpoint hiện tại có 8 (N) instance ml.g5.12xlarge, có 4 GPU accelerator của mỗi instance. Bạn đặt kích thước batch tối đa thành 2 (Z) bản sao, và mỗi bản sao cần 1 (Y) GPU accelerator. Giá trị service quota AWS tối thiểu cho ml.g5.12xlarge là ROUNDUP(2 x 1 / 4) + 8 = 9. Trong một tình huống khác, khi mỗi bản sao của inference component yêu cầu 4 GPU accelerator, thì service quota cấp tài khoản cần thiết cho cùng một instance phải là ROUNDUP(2 x 4 / 4) + 8 = 10.\nKết luận Các bản cập nhật cuốn chiếu (rolling updates) cho các thành phần suy luận đại diện cho một bước tiến quan trọng trong khả năng triển khai của SageMaker AI. Tính năng này trực tiếp giải quyết các thách thức trong việc cập nhật mô hình đang chạy trong môi trường sản xuất, đặc biệt là với những khối lượng công việc nặng về GPU. Nó giúp loại bỏ việc phải ước lượng dung lượng thủ công, đồng thời giảm thiểu rủi ro khi cần hoàn tác (rollback).\nBằng cách kết hợp quy trình cập nhật theo lô (batch-based updates) cùng với các biện pháp bảo vệ tự động, SageMaker AI đảm bảo rằng các quá trình triển khai luôn linh hoạt và ổn định.\nCác lợi ích chính bao gồm:\nGiảm chi phí tài nguyên trong quá trình triển khai, không cần cấp phát thêm cụm máy chủ dự phòng. Cải thiện cơ chế bảo vệ trong quá trình triển khai nhờ cập nhật dần và khả năng tự động hoàn tác khi xảy ra lỗi. Duy trì khả năng hoạt động liên tục trong khi cập nhật, với kích thước lô có thể cấu hình. Đơn giản hóa việc triển khai các mô hình phức tạp cần nhiều bộ tăng tốc (multi-accelerator models). Dù bạn đang triển khai mô hình nhỏ gọn hay các mô hình lớn sử dụng nhiều bộ tăng tốc, rolling updates mang lại một phương pháp hiệu quả hơn, tiết kiệm chi phí và an toàn hơn để giữ cho các mô hình máy học của bạn luôn được cập nhật trong môi trường sản xuất.\nChúng tôi khuyến khích bạn dùng thử khả năng mới này trên các endpointSageMaker AI của mình và khám phá cách nó có thể nâng cao hiệu quả hoạt động ML của bạn. Để biết thêm thông tin, vui lòng tham khảo SageMaker AI documentation hoặc liên hệ với nhóm tài khoản AWS của bạn.\nVề các tác giả Melanie Li, Tiến sĩ, là Chuyên gia Kiến trúc Giải pháp Generative AI Cấp cao tại AWS, trụ sở tại Sydney, Úc. Cô tập trung vào việc hỗ trợ khách hàng xây dựng các giải pháp ứng dụng các công cụ AI và máy học tiên tiến nhất. Cô đã tham gia vào nhiều sáng kiến Generative AI trên toàn khu vực APJ, tận dụng sức mạnh của Large Language Models (LLMs). Trước khi gia nhập AWS, Tiến sĩ Li từng đảm nhiệm vai trò nhà khoa học dữ liệu trong lĩnh vực tài chính và bán lẻ.\nAndrew Smith là Kỹ sư Hỗ trợ Đám mây trong nhóm SageMaker, Vision \u0026amp; Other tại AWS, trụ sở Sydney, Úc. Anh hỗ trợ khách hàng sử dụng nhiều dịch vụ AI/ML của AWS, đặc biệt có chuyên môn sâu về Amazon SageMaker. Ngoài công việc, anh thích dành thời gian cho gia đình, bạn bè và tìm hiểu các công nghệ mới.\nDustin Liu là Kiến trúc sư Giải pháp (Solutions Architect) tại AWS, tập trung hỗ trợ các công ty khởi nghiệp và doanh nghiệp SaaS trong lĩnh vực dịch vụ tài chính và bảo hiểm (FSI). Anh có nền tảng đa dạng trong kỹ thuật dữ liệu (data engineering), khoa học dữ liệu (data science) và máy học (machine learning), đồng thời đam mê ứng dụng AI/ML để thúc đẩy đổi mới và chuyển đổi doanh nghiệp.\nVivek Gangasani là Chuyên gia Kiến trúc Giải pháp Generative AI Cấp cao tại AWS. Anh giúp các công ty khởi nghiệp về Generative AI xây dựng các giải pháp sáng tạo bằng cách sử dụng dịch vụ AWS và hạ tầng tính toán tăng tốc. Hiện tại, anh tập trung vào việc phát triển các chiến lược tinh chỉnh (fine-tuning) và tối ưu hiệu năng suy luận (inference performance) cho các mô hình ngôn ngữ lớn (LLMs). Ngoài công việc, Vivek thích leo núi, xem phim và khám phá ẩm thực.\nShikher Mishra là Kỹ sư Phát triển Phần mềm (Software Development Engineer) thuộc nhóm SageMaker Inference, với hơn 9 năm kinh nghiệm trong ngành. Anh đam mê xây dựng các giải pháp mở rộng, hiệu quả, giúp khách hàng triển khai và quản lý ứng dụng máy học một cách dễ dàng. Thời gian rảnh, Shikher yêu thích thể thao ngoài trời, leo núi và du lịch.\nJune Won là Quản lý sản phẩm (Product Manager) của Amazon SageMaker JumpStart. Anh tập trung vào việc giúp khách hàng dễ dàng tìm kiếm và sử dụng các mô hình nền tảng (foundation models) để xây dựng ứng dụng Generative AI. Kinh nghiệm của anh tại Amazon còn bao gồm phát triển ứng dụng mua sắm di động và giải pháp giao hàng chặng cuối (last-mile delivery).\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Thông báo kết thúc hỗ trợ và khả năng khám phá nâng cao cho Amazon EKS Ngày 05 tháng 03 năm 2025\nTác giả: Praseeda Sathaye (Principal Solutions Architect, Containers \u0026amp; OSS), AJ Davis (AWS Enterprise Support) và Arvind Viswanathan (Principal Solutions Architect)\nGiới thiệu Trong thế giới ứng dụng container hóa đang phát triển nhanh chóng, việc duy trì khả năng phục hồi và khả năng quan sát trên các môi trường Kubernetes đã trở thành một thách thức quan trọng. Khi các tổ chức ngày càng áp dụng Amazon Elastic Kubernetes Service (Amazon EKS) để quản lý khối lượng công việc container hóa của họ, nhu cầu về quản lý vòng đời phiên bản cluster và cơ chế khám phá trở nên cực kỳ quan trọng. Khi môi trường Amazon EKS trở nên phức tạp hơn và mở rộng trên nhiều AWS Regions và tài khoản, người dùng thường gặp khó khăn trong việc theo dõi phiên bản cluster, vòng đời hỗ trợ và trạng thái triển khai tổng thể.\nGiám sát chủ động vòng đời cluster EKS và kết thúc hỗ trợ là rất quan trọng để đảm bảo tính bảo mật, ổn định và tuân thủ của các triển khai Kubernetes. Hơn nữa, việc có được khả năng hiển thị các triển khai cluster EKS trên toàn bộ AWS Organization là điều cần thiết cho việc quản lý tài nguyên hiệu quả, lập kế hoạch chiến lược và duy trì bảng kiểm kê chính xác.\nTrong bài viết này, để giải quyết những điểm yếu này, chúng tôi chia sẻ hai giải pháp mạnh mẽ cung cấp khả năng quan sát các cluster EKS:\nThông báo kết thúc hỗ trợ Khám phá và báo cáo Giải pháp đầu tiên sử dụng AWS Health, Amazon EventBridge và Amazon Simple Notification Service (Amazon SNS)/Amazon Simple Queue Service (Amazon SQS) để giám sát các sự kiện cụ thể của Amazon EKS, đặc biệt đối với các cluster sắp kết thúc hỗ trợ (tiêu chuẩn và mở rộng). Việc cung cấp thông báo sớm khi một cluster EKS sắp kết thúc cửa sổ hỗ trợ cho phép giải pháp này trao quyền cho bạn chủ động lập kế hoạch và cập nhật phiên bản Kubernetes của cluster.\nBổ sung cho điều này, giải pháp thứ hai là một cơ chế khám phá và báo cáo tự động xác định và tổng hợp thông tin chi tiết về các cluster EKS trên tất cả các AWS Regions và tài khoản trong Organization của bạn. Khả năng hiển thị toàn diện này về các phiên bản cluster, tags liên quan và các chi tiết chính khác giúp kiểm tra tuân thủ, quản lý bảng kiểm kê tài nguyên chính xác và lập kế hoạch nâng cấp chiến lược.\nCùng nhau, hai giải pháp này cung cấp một framework mạnh mẽ để quản lý vòng đời cluster EKS hiệu quả, cho phép các tổ chức luôn đi trước các vấn đề tiềm ẩn, tối ưu hóa việc sử dụng tài nguyên và đưa ra các quyết định sáng suốt phù hợp với mục tiêu chiến lược dài hạn của họ.\nĐiều kiện tiên quyết Bạn cần những điều sau để hoàn thành hướng dẫn:\nMột AWS account với Organizations được bật Gói hỗ trợ Business, Enterprise On-Ramp hoặc Enterprise từ AWS Support để sử dụng AWS Health API Kiến thức cơ bản về Amazon EKS, AWS Health, EventBridge, AWS Lambda, AWS Identity and Access Management (IAM), Amazon S3, Amazon SNS, Amazon SQS và AWS Cloud Development Kit (AWS CDK) Khả năng ủy quyền từ tài khoản quản lý đến tài khoản tooling được sử dụng để tập trung thông báo và thực hiện khám phá cluster EKS trên toàn bộ Organization Kiến thức về Python Thiết lập ban đầu Các bước sau hướng dẫn bạn qua quá trình thiết lập ban đầu.\nBật AWS Health Organizational View trong tài khoản quản lý Bật Organizational View in AWS Health để có được chế độ xem tập trung, tổng hợp các sự kiện AWS Health trên toàn bộ Organization của bạn. Bạn có thể xác minh rằng điều này được bật thông qua console hoặc bằng cách chạy lệnh sau bằng AWS Command Line Interface (AWS CLI):\naws health describe-health-service-status-for-organization. Bạn sẽ thấy kết quả sau:\n{\u0026ldquo;healthServiceAccessStatusForOrganization\u0026rdquo;: \u0026ldquo;ENABLED\u0026rdquo; }\nGói hỗ trợ Business, Enterprise On-Ramp hoặc Enterprise từ AWS Support là cần thiết để sử dụng AWS Health API và hoàn thành bước này.\nỦy quyền quản trị từ tài khoản quản lý đến tài khoản tooling trung tâm Thiết lập một tài khoản AWS trong Organization làm tài khoản tooling cho giải pháp này. Tài khoản này được sử dụng để tập trung thông báo và khám phá.\nTừ tài khoản quản lý, ủy quyền quản trị AWS CloudFormation StackSets bằng cách làm theo các bước được mô tả trong bài viết này: CloudFormation StackSets delegated administration.\nKết quả tương tự cũng có thể đạt được bằng cách chạy lệnh sau từ tài khoản quản lý. Thay thế 012345678901 bằng AWS account ID của tài khoản tooling của bạn.\naws organizations register-delegated-administrator \\\n\u0026ndash;serviceprincipal=member.org.stacksets.cloudformation.amazonaws.com \\\n\u0026ndash;account-id=\u0026ldquo;012345678901\u0026rdquo;\nĐây là lần duy nhất chúng ta cần truy cập tài khoản quản lý. Các bước còn lại được hoàn thành từ trong tài khoản tooling.\nBootstrap AWS CDK Chọn một Region chính nơi tất cả báo cáo và sự kiện được hợp nhất trong tài khoản tooling trung tâm. Đặt biến AWS_DEFAULT_REGION thành Region chính này.\nĐối với giải pháp khám phá và báo cáo, bạn phải bootstrap AWS CDK trong Region chính này trên toàn bộ Organization. Hơn nữa, AWS CDK cũng phải được bootstrap trong tất cả các AWS Regions nơi các cluster EKS được triển khai để nhận thông báo kết thúc hỗ trợ. Để đơn giản hóa hướng dẫn này, chúng tôi chỉ demo triển khai tài nguyên đến Region chính mà bạn đã chọn.\nCác bước để bootstrap AWS CDK trên nhiều AWS Regions và tài khoản có sẵn trong bài viết này: Bootstrapping multiple AWS accounts for AWS CDK using CloudFormation StackSets.\nTải xuống các AWS CDK stacks Chúng tôi cung cấp các AWS CDK stacks để bạn nhanh chóng triển khai giải pháp trong môi trường của mình. Tải mã từ our GitHub repository và thiết lập môi trường bằng cách chạy các lệnh sau trong thư mục cdk:\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nHướng dẫn chi tiết Các bước sau sẽ hướng dẫn bạn qua các giải pháp này.\nGiải pháp 1: Thông báo kết thúc hỗ trợ cluster EKS Giải pháp đầu tiên của chúng tôi giải quyết nhu cầu quan trọng về nhận thức kịp thời về các sự kiện vòng đời cluster EKS, đặc biệt là việc tiếp cận ngày kết thúc hỗ trợ tiêu chuẩn. Việc sử dụng AWS Health, EventBridge và Amazon SNS (và tùy chọn Amazon SQS) cho phép chúng tôi tạo một hệ thống tập trung:\nGiám sát các sự kiện AWS Health trên nhiều AWS Regions và tài khoản Tập trung vào các sự kiện cụ thể của Amazon EKS, đặc biệt là AWS_EKS_PLANNED_LIFECYCLE_EVENT Cung cấp thông báo sớm khi một cluster EKS còn 180 ngày nữa sẽ đạt đến cuối giai đoạn hỗ trợ tiêu chuẩn và hỗ trợ mở rộng Cách tiếp cận tập trung này đảm bảo rằng người dùng Amazon EKS nhận được đủ thời gian để lập kế hoạch và thực hiện nâng cấp phiên bản, duy trì tính bảo mật và ổn định của môi trường Kubernetes của họ, như được hiển thị trong hình sau.\nHình 1: Tổng quan giải pháp – thông báo kết thúc hỗ trợ\nBước 1: Triển khai AWS CDK stack eks-health-events Triển khai AWS CDK stack eks-health-events vào tài khoản tooling trung tâm bằng lệnh sau:\ncdk deploy eks-health-events \u0026ndash;app \u0026ldquo;python3 tooling_account.py\u0026rdquo; —require-approval never\nĐiều này triển khai AWS CDK app trong tooling_account.py, cung cấp các tài nguyên sau trong tài khoản tooling trung tâm:\nEvent bus SNS topic và SQS queue để giám sát các sự kiện EventBridge rule để chuyển tiếp các sự kiện vòng đời đã lên kế hoạch cho Amazon EKS đến Amazon SNS EventBridge rule để chuyển tiếp giám sát các sự kiện vòng đời đã lên kế hoạch cho Amazon EKS đến Amazon SQS Resource policies cho các event rules để publish đến Amazon SNS và Amazon SQS Bước 2: Triển khai AWS CDK stack eks-health-events-stack-set Triển khai AWS CDK stack eks-health-events-stack-set.\ncdk deploy eks-health-events-stack-set \u0026ndash;app \u0026ldquo;python stack_sets.py\u0026rdquo; —require-approval never\nĐiều này sử dụng CloudFormation StackSets để triển khai các tài nguyên sau vào Region chính đã chọn trên tất cả các tài khoản trong Organization ngoại trừ tài khoản Management:\nLocal event bus EventBridge rule để chuyển tiếp các sự kiện vòng đời đã lên kế hoạch cho Amazon EKS đến central event bus được cung cấp trong Bước 2 Resource policies cho các event rules để publish đến central event bus Bước 3: Cấu hình thông báo SNS Duyệt đến dịch vụ Amazon SNS có tên eks-health-events-EKSHealthEvents- và tạo một subscription cho topic mới được tạo (ví dụ: địa chỉ email nhóm).\nBước 4: Xác thực giải pháp Bạn có thể kiểm tra và xác thực rằng các EventBridge rules, SQS queue và SNS topic đã được tạo bởi các CloudFormation stacks có tên eks-health-events và eks-health-events-stack-set. Từ thời điểm này trở đi, khi các cluster EKS của bạn còn 180 ngày nữa sẽ đạt đến cuối hỗ trợ (tiêu chuẩn và mở rộng), các EventBridge rules sẽ áp dụng và Amazon SNS và/hoặc Amazon SQS được kích hoạt, như được hiển thị trong các hình sau.\nHình 2: Xác thực triển khai EventBridge\nHình 3: Xác thực triển khai SQS\nHình 4: Xác thực triển khai SNS\nHình 5: Mẫu thông báo kết thúc hỗ trợ\nGiải pháp 2: Khám phá và báo cáo cluster EKS Bổ sung cho giải pháp thông báo kết thúc hỗ trợ cluster EKS, giải pháp thứ hai của chúng tôi cung cấp cái nhìn toàn diện về các cluster EKS trên toàn bộ Organization. Giải pháp này:\nXác định các cluster EKS trong tất cả các AWS Regions và tài khoản trong một Organization Thu thập thông tin chi tiết về từng cluster, chẳng hạn như chi tiết tài khoản, region, tên cluster, phiên bản và các tags liên quan Tổng hợp dữ liệu về các phiên bản cluster, cung cấp thông tin chi tiết về phân phối phiên bản Tạo cả báo cáo chi tiết và tóm tắt, được lưu trữ tập trung để truy cập trực tiếp Việc cung cấp khả năng hiển thị trên toàn tổ chức này cho phép giải pháp giúp các nhóm duy trì bảng kiểm kê chính xác về tài nguyên Amazon EKS, tạo điều kiện thuận lợi cho việc kiểm tra tuân thủ và hỗ trợ lập kế hoạch nâng cấp chiến lược, như được hiển thị trong hình sau.\nHình 6: Tổng quan giải pháp – khám phá và báo cáo\nBước 1: Triển khai AWS CDK stack eks-discovery Triển khai AWS CDK stack eks-discovery-lambda vào tài khoản tooling trung tâm bằng lệnh sau:\ncdk deploy eks-discovery-lambda —require-approval never\nĐiều này triển khai AWS CDK stack có tên eks-discovery-lambda trong tooling_account.py, cung cấp các tài nguyên sau trong tài khoản tooling trung tâm:\nLambda function để khám phá các cluster EKS trên tất cả các AWS Regions và tài khoản S3 bucket để lưu trữ kết quả SNS topic cho thông báo EventBridge scheduler để thực thi định kỳ Các IAM roles và policies cần thiết Lambda function thu thập chi tiết cluster, tạo báo cáo và gửi thông báo.\nBước 2: Sửa đổi EventBridge scheduler theo nhu cầu Nếu bạn muốn tùy chỉnh lịch khám phá cluster EKS, hãy điều hướng đến EventBridge và trong phần schedules tìm EKSDiscoveryWeeklySchedule mới được tạo. Đây là một scheduler dựa trên cron, như được hiển thị trong hình sau.\nHình 7: Tùy chỉnh lịch cho khám phá cluster\nĐể nhận thông báo từ Amazon SNS, bạn phải tạo một subscription cho topic. Để làm điều này, hãy điều hướng đến dịch vụ Amazon SNS, tìm Topic mới được tạo có tên EKSDiscoverySNSTopic và cấu hình giao thức để đáp ứng yêu cầu của bạn (ví dụ: gửi email đến một nhóm).\nBước 3: Triển khai cross-account role mà Lambda function có thể assume để thực hiện khám phá Lambda function bạn triển khai trong Bước 1 dựa vào một cross-account role trong mỗi tài khoản trong Organization để thực hiện khám phá cluster.\nTriển khai AWS CDK stack eks-discovery-stack-set để triển khai cross-account role này.\ncdk deploy eks-discovery-stack-set \u0026ndash;app \u0026ldquo;python stack_sets.py\u0026rdquo; \u0026ndash;require-approval never\nBước 4: Xác thực giải pháp Để xác thực giải pháp, hãy điều hướng đến Lambda function mới được tạo và test với một event mới và một đối tượng JSON rỗng. Khi Lambda hoàn thành, hãy xác minh rằng S3 bucket nhận được file zip và xác nhận rằng bạn đã nhận được thông báo SNS, như được hiển thị trong các hình sau.\nHình 8: Mẫu output khám phá cluster trong S3 bucket\nHình 9: Mẫu nội dung của output file\nHình 10: Mẫu danh sách clusters\nHình 11: Mẫu số lượng clusters theo phiên bản\nBước 5: (Tùy chọn) Giám sát giải pháp Bạn có thể muốn giám sát giải pháp. Điều này có thể được thực hiện bằng cách thiết lập Amazon CloudWatch Alarms để giám sát việc thực thi của Lambda function và bất kỳ lỗi tiềm ẩn nào. Hơn nữa, hãy thường xuyên xem xét các báo cáo được tạo trong S3 bucket và định kỳ xem xét và cập nhật các quyền IAM nếu cần.\nKhắc phục sự cố Đảm bảo rằng tất cả các IAM roles và policies được thiết lập chính xác và có các quyền cần thiết. Kiểm tra CloudWatch Logs để tìm bất kỳ thông báo lỗi nào trong các Lambda functions hoặc EventBridge rules. Cân nhắc về bảo mật Xem xét và điều chỉnh các IAM roles và policies để tuân thủ nguyên tắc đặc quyền tối thiểu và môi trường của bạn. Thường xuyên kiểm toán quyền truy cập vào hệ thống quản lý sự kiện tập trung. Dọn dẹp Chạy các lệnh sau để dọn dẹp các tài nguyên đã cung cấp:\ncdk destroy \u0026ndash;app \u0026ldquo;python stack_sets.py\u0026rdquo; \u0026ndash;all \u0026ndash;force\ncdk destroy \u0026ndash;all \u0026ndash;force\nLệnh đầu tiên xóa các CloudFormation StackSets đã được triển khai trên toàn bộ Organization bằng AWS CDK App có tên stack_sets.py.\nLệnh thứ hai dọn dẹp các tài nguyên được cung cấp trong tài khoản tooling trung tâm bằng AWS CDK App có tên tooling_account.py.\nKết luận Hướng dẫn này có thể giúp bạn thiết lập một hệ thống mạnh mẽ sử dụng các dịch vụ AWS để cung cấp thông báo chủ động về kết thúc hỗ trợ tiêu chuẩn. Điều này cho phép lập kế hoạch kịp thời cho các nâng cấp, giảm thiểu rủi ro từ các cluster lỗi thời trong khi duy trì tính bảo mật, ổn định và tuân thủ. Hơn nữa, giải pháp khám phá và báo cáo cluster Amazon EKS đánh dấu một bước tiến đáng kể trong việc quản lý các môi trường Kubernetes đa tài khoản phức tạp trên AWS. Giải pháp tăng cường khả năng hiển thị, hợp lý hóa nỗ lực tuân thủ, tạo điều kiện thuận lợi cho việc lập kế hoạch chiến lược và hỗ trợ ra quyết định sáng suốt cho việc nâng cấp cluster và phân bổ tài nguyên.\nKhi các tổ chức tiếp tục mở rộng quy mô ứng dụng container hóa của họ, các giải pháp này trở thành tài sản vô giá. Chúng cho phép các nhóm duy trì cái nhìn tổng quan rõ ràng về cảnh quan Amazon EKS của họ, tối ưu hóa việc sử dụng tài nguyên và đảm bảo các thực hành quản lý nhất quán trên các triển khai đa dạng. Việc triển khai các giải pháp này cho phép bạn thực hiện một bước tiến đáng kể trong việc quản lý khả năng quan sát, khả năng phục hồi và quản trị của các môi trường Amazon EKS của bạn. Đến lượt nó, điều này đảm bảo thành công lâu dài và khả năng mở rộng của các sáng kiến Kubernetes của bạn trên AWS.\nChúng tôi khuyến nghị thử cả hai giải pháp để bắt đầu tăng cường khả năng quan sát cluster EKS của bạn ngay hôm nay!\nNguồn gốc: AWS Containers Blog Ngày xuất bản: 12 tháng 3, 2025 Danh mục: Amazon EKS, AWS Health, Container Management, Kubernetes\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deepen understanding of AWS IAM access control: Users, Groups, Policies, and Roles. Learn how to design and implement secure access patterns using IAM Roles and least privilege. Practice creating IAM Groups, Users, Roles and configuring role switching scenarios. Understand core Amazon VPC networking concepts: Subnets, Route Tables, Internet Gateway, NAT Gateway. Get hands-on experience with VPC security (Security Groups, Network ACLs) and an introduction to Site-to-Site VPN. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study IAM access control overview + IAM Users \u0026amp; IAM Groups + IAM Policies + IAM Roles - Review security objectives and principle of least privilege 09/23/2025 09/23/2025 https://000002.awsstudygroup.com/ 2 - Hands-on IAM (1): + Create Admin IAM Group + Create Admin User and add to group + Login as Admin User and verify permissions 09/24/2025 09/24/2025 https://000002.awsstudygroup.com/ 3 - Hands-on IAM (2): + Create Admin Role + Create OperatorUser + Configure trust relationships for role switching + Test “Switch Role” from OperatorUser + Review and clean up config 09/25/2025 09/25/2025 https://000002.awsstudygroup.com/ 4 - Study Amazon VPC fundamentals: + Subnets + Route Tables + Internet Gateway + NAT Gateway - Learn VPC security components: + Security Groups + Network ACLs 09/26/2025 09/26/2025 https://000003.awsstudygroup.com/ 5 - Hands-on VPC \u0026amp; Networking: + Create VPC, Subnets, Internet Gateway, Route Tables, Security Groups + Enable VPC Flow Logs + Launch EC2 instance in VPC and test connectivity + Read through Site-to-Site VPN setup steps 09/27/2025 09/27/2025 https://000003.awsstudygroup.com/ Week 3 Achievements: Gained a solid understanding of IAM access control concepts:\nIAM Users and IAM Groups IAM Policies (permission-based access) IAM Roles and trust relationships Implemented a basic IAM administrative structure:\nCreated Admin Group and Admin User Verified permissions using group-based policies Practiced advanced IAM patterns:\nCreated Admin Role and OperatorUser Configured and tested “Switch Role” flows Applied the principle of least privilege when assigning permissions Understood Amazon VPC core components:\nSubnets, Route Tables, Internet Gateway, NAT Gateway Security Groups vs. Network ACLs Built a small VPC environment:\nCreated VPC, subnets, routing, and security layers Launched EC2 instance inside VPC and verified connectivity Enabled VPC Flow Logs for traffic visibility Reviewed the overall steps to set up AWS Site-to-Site VPN for hybrid connectivity.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.3-bedrock-models/","title":"Activate Bedrock Models","tags":[],"description":"","content":"Before deploying the solution, you need to activate the necessary Amazon Bedrock models in your AWS account.\nSteps to Activate Models Search for Amazon Bedrock in AWS Console Access Model catalog from the left navigation menu Select the corresponding model names: Anthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Cohere - Embed Multilingual v3 Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to activate each model Note: Make sure you activate all four models in the ap-southeast-1 (Singapore) region as the solution is deployed in this region.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Purpose of the Workshop Provide an overview of the AI/ML/GenAI ecosystem on AWS, focusing on the most important services. Equip attendees with practical knowledge of the AI/ML workflow—from data preparation to model training and deployment using Amazon SageMaker. Introduce the application of Generative AI with Amazon Bedrock, including prompt engineering, RAG, and building automation agents. Create an environment for students and developers to connect and exchange real-world AI/ML experience on the cloud. Speakers / Instructors (According to the workshop program) Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Trần Thị Minh Anh – Senior AI/ML Specialist Solutions Architect, AWS Lê Quang Huy – Cloud Engineer \u0026amp; AI Enthusiast Key Content \u0026amp; Learnings 1. Overview of AWS AI/ML/GenAI Ecosystem Vietnam landscape: AI adoption is growing rapidly; demand for AI/ML and cloud-skilled engineers continues to rise. Learning roadmap: From foundational courses to advanced certifications, emphasizing hands-on practice. Three service groups: AI Services (ready-to-use), ML Services (customizable), and Generative AI (opening new possibilities). 2. Amazon SageMaker – A Complete ML Platform Supports the entire ML lifecycle in a single platform. SageMaker Studio provides an integrated development environment that accelerates model development. Enables MLOps, training automation, hyperparameter tuning, and model deployment. 3. Generative AI with Amazon Bedrock Offers multiple Foundation Models such as Claude, Llama, and Titan for different tasks. Advanced Prompt Engineering: using Chain-of-Thought, few-shot techniques for improved output quality. RAG (Retrieval-Augmented Generation): combines LLMs with private data to reduce hallucination and expand knowledge scope. Bedrock Agents: enable models to perform multi-step tasks and interact with external systems. Guardrails: ensure safe content and compliance. Key Takeaways Technology Insights SageMaker is not just a training tool but a comprehensive ML lifecycle management platform. Effective Generative AI relies on the combination of models, prompt techniques, and supporting architectures like RAG. Data remains the most critical factor influencing model quality. Mindset \u0026amp; Methodology Start with real-world problems instead of chasing technology trends. Apply an iterative approach: fast prototyping → gather feedback → improve. Always evaluate cost to ensure project feasibility. Practical Skills Prompt engineering requires continuous practice. Proficiency in AWS Console/SDK is essential for building real-world AI products. Personal Action Plan After the Workshop Practice using AWS Free Tier — start with SageMaker Studio Lab to explore ML workflows. Build a first GenAI application — for example, creating a simple chatbot using Amazon Bedrock. Deepen learning through AWS Skill Builder — complete learning paths for ML and Generative AI. Join Vietnam AI/ML communities to stay updated with case studies and hands-on knowledge. Personal Experience at the Event The “AI/ML/GenAI on AWS” workshop provided a clearer and more practical perspective on how to approach AI in a professional direction. The modern AWS office environment, the enthusiasm of the speakers, and their easy-to-understand explanations made the content very engaging.\nThe live demo was the most impressive part. Seeing Ms. Minh Anh build a working RAG system and Bedrock Agent in a short time made me realize how accessible AI prototyping has become when using the right tools.\nInsights about cost management and MLOps were especially valuable—they helped me understand that AI is not just about building a model, but also about managing, monitoring, and optimizing it continuously.\nAfter the workshop, I feel more confident in pursuing AI/ML on AWS, as I now have a clearer and more practical roadmap to follow.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Purpose of the Workshop Provide a comprehensive overview of DevOps culture, principles, and practices on AWS. Demonstrate how to build a complete CI/CD pipeline—from source control to automated deployment. Introduce major Infrastructure as Code (IaC) tools on AWS and their practical applications. Equip attendees with knowledge on containerization, observability, and operational best practices. Speakers / Main Instructors (Expected) Đỗ Huy Thắng – DevOps Lead, VNG Nguyễn Thị Thu Hà – Senior DevOps Engineer, AWS Phạm Tuấn Anh – Solutions Architect, AWS Key Topics \u0026amp; Learnings 1. DevOps Culture \u0026amp; Core Principles DevOps is not just about tools—it represents a collaborative culture between Development and Operations. DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) are critical indicators of DevOps performance. Shift-left Testing \u0026amp; Security helps detect issues earlier and reduce remediation cost. 2. CI/CD Pipeline on AWS AWS DevTools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) enable fully automated release workflows. Deployment strategies include Blue/Green, Canary, and Rolling Updates. Live demonstration: commit → build → test → deploy to EC2/ECS. 3. Infrastructure as Code (IaC) AWS CloudFormation: declarative IaC using YAML/JSON templates. AWS CDK: imperative IaC using languages like Python or TypeScript, offering strong reusability. Drift Detection ensures infrastructure remains consistent with defined configurations. 4. Containers \u0026amp; Microservices Docker packages applications into lightweight, portable, isolated containers. Comparison of Amazon ECS vs Amazon EKS to choose the right orchestration tool. AWS App Runner offers serverless container deployment with minimal operational overhead. 5. Monitoring \u0026amp; Observability Amazon CloudWatch: metrics, logs, alarms, dashboards for real-time system health. AWS X-Ray: distributed tracing to analyze performance and identify latency bottlenecks. Best practices include meaningful alerts, dashboards aligned with SLOs, and a structured postmortem process. Key Takeaways Mindset \u0026amp; Culture DevOps is a continuous improvement journey. Automation is the backbone of DevOps. Adopt a “blameless postmortem” mindset to learn from failures. Technical Knowledge CI/CD is about creating a reliable delivery workflow, not just running scripts. IaC enables consistency, version control, and environment reproducibility. Observability provides deeper insights into root cause analysis. Career Skills DevOps engineers need broad knowledge: networking, security, coding, system operations. Advanced debugging skills are essential for distributed systems. Personal Action Plan Build a personal CI/CD pipeline using CodeCommit \u0026amp; CodePipeline. Practice AWS CDK using Python to deploy core AWS resources. Package a small application into Docker and deploy it on ECS Fargate. Create a CloudWatch dashboard with essential metrics and alarms. Personal Reflection This workshop offered highly practical insights that developers often overlook.\nThe CI/CD and IaC demonstrations were particularly impressive, showing how automation reduces risk and accelerates release cycles.\nReal-world case studies from VNG and AWS emphasized the critical role of monitoring and DevOps culture.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 - 17:00 , September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00 - 16:30, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:00 - 11:3, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 8:30 - 17:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 8:30 - 12:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: Club Session – \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Purpose of the Session Explore how Generative AI is transforming modern software development. Understand how AI can be fully integrated into the Software Development Life Cycle (SDLC) from architecture to maintenance. Experience hands-on examples of AI-powered development tools such as Amazon Q Developer and Kiro. Speakers \u0026amp; Organizers Main Facilitators: Toan Huynh – Presented AI-Driven SDLC and demoed Amazon Q Developer My Nguyen – Demonstrated Kiro Coordinators: Diem My, Dai Truong, Dinh Nguyen (AWS GenAI Builder Club Vietnam) Key Content \u0026amp; Learnings 1. Overview of the AI-Driven Development Life Cycle Generative AI is reshaping the entire SDLC, not just assisting with small tasks. AI automates repetitive work such as boilerplate coding, generating test cases, and basic debugging. Developers are shifting from writing every line of code to designing, supervising, and collaborating with AI. 2. Amazon Q Developer – An End-to-End AI Developer Assistant Supports all stages of SDLC: planning, requirement analysis, architecture design coding, code explanation, refactoring, language migration debugging and testing deployment guidance and troubleshooting Deep AWS integration enhances productivity for cloud-native workloads. Built with strong privacy principles—enterprise data is not used to train global models. 3. Kiro – Practical AI Pair Programming Understands complex codebases and provides intelligent modification suggestions. Accelerates development by generating accurate, production-ready code. Acts as a learning companion to quickly explore new languages or frameworks. Key Takeaways Trends \u0026amp; Future Direction AI will not replace developers; instead, it elevates their role toward problem-solving, system design, and quality assurance. Speed becomes a competitive advantage for organizations that adopt AI across the SDLC. Skills to Develop Prompt engineering becomes essential: specifying requirements clearly and validating AI-generated output. Architectural thinking and oversight skills are needed to evaluate AI-generated code safely. Strong fundamentals remain crucial: algorithms, data structures, and system design enable developers to guide AI effectively. Personal Action Plan Register and use Amazon Q Developer in VSCode/JetBrains for daily tasks. Practice prompt engineering for coding scenarios—code generation, testing, debugging. Engage actively with the AWS GenAI Builder Club community. Apply an AI-driven development workflow to a real module in a personal or team project. Personal Reflection This club session felt like witnessing the next evolution of software engineering.\nSeeing Amazon Q Developer and Kiro operate in real time—proposing improvements, detecting potential vulnerabilities, and converting natural language prompts into working code—was truly eye-opening.\nThe most impactful message was: “AI empowers, not replaces.”\nDevelopers who learn to collaborate with AI will have a tremendous advantage in the future.\nThis session encouraged me to be proactive in mastering AI tools and to integrate them into my workflow as an essential part of becoming a next-generation software engineer.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand how to launch and manage both Windows and Linux EC2 instances. Learn how to deploy applications on EC2 (Node.js \u0026amp; AWS User Management App). Understand IAM governance and cost usage governance in EC2 environments. Learn how applications securely access AWS services using IAM roles instead of access keys. Practice attaching IAM roles to EC2 and testing application access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study EC2 introduction \u0026amp; preparation steps + EC2 concepts + Resources required + IAM \u0026amp; networking prerequisites 09/30/2025 09/30/2025 https://000004.awsstudygroup.com/ 2 - Launch EC2 instances: + Launch Windows Server 2022 instance + Launch Amazon Linux instance + Connect and validate OS-level access 10/01/2025 10/01/2025 https://000004.awsstudygroup.com/ 3 - Deploy applications on EC2: + Deploy AWS User Management App on Amazon Linux + Deploy Node.js application on Windows EC2 10/02/2025 10/02/2025 https://000004.awsstudygroup.com/ 4 - IAM Governance \u0026amp; Authorization: + Understand cost \u0026amp; usage governance with IAM + Compare Access Key vs IAM Role + Learn risks of long-term access keys 10/03/2025 10/03/2025 https://000048.awsstudygroup.com/ 5 - IAM Role Hands-on: + Create IAM Role for EC2 + Attach IAM Role to EC2 instance + Test application accessing AWS services via IAM Role + Clean up resources 10/04/2025 10/04/2025 https://000048.awsstudygroup.com/ Week 4 Achievements: Successfully launched both Windows and Linux EC2 instances. Understood how to prepare and configure an EC2 environment for application deployment. Deployed two applications: AWS User Management Application on Linux Node.js Application on Windows Learned about IAM governance and how permissions affect cost and security. Understood why Access Keys should not be used for applications. Created and attached IAM Roles to EC2 for secure access to AWS services. Verified application access using IAM Role-based credentials. Cleaned up all unused resources to avoid unnecessary costs. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.4-aws-cli/","title":"Configure AWS CLI","tags":[],"description":"","content":"To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your authentication information.\nSteps Step 1: Check if AWS CLI is installed aws --version If not installed:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Step 2: Create IAM User (if not already exists) Login to AWS Console Search \u0026ldquo;IAM\u0026rdquo; → Click IAM Left sidebar → Users → Create user User name: arc-workshop-user Click Next Attach policies directly, select policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Click Create user Step 3: Create Access Key Go to the created user → Tab Security credentials Scroll down Access keys → Click Create access key Select Command Line Interface (CLI) Tick \u0026ldquo;I understand\u0026hellip;\u0026rdquo; → Next Description: ARC Workshop CLI Click Create access key ⚠️ IMPORTANT: Copy or download .csv file\nAccess key ID: AKIAXXXXXXXXXXXXXXXX Secret access key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Step 4: Configure AWS CLI Open PowerShell and run:\naws configure Enter information:\nAWS Access Key ID [None]: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name [None]: ap-southeast-1 Default output format [None]: json Step 5: Verify Configuration Check identity:\naws sts get-caller-identity Expected output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/arc-workshop-user\u0026#34; } "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Next Step: Create Your Workspace \u0026amp; Connect Your Cloud Your workspace is the control center where you\u0026rsquo;ll:\nConnect cloud accounts Monitor spending Get AI-powered savings recommendations "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn the core components of EC2 and how compute operates in AWS. Understand Auto Scaling, EBS, Instance Store, User Data, Metadata. Practice backup, Storage Gateway, and deploying EC2 for storage-related use cases. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn about EC2, instance types, AMI, key pair - Understand EBS, Instance Store, User Data, Metadata 06/10/2025 06/10/2025 https://youtu.be/-t5h4N6vfBs?si=GeVdhO9IEDjzzS_D https://youtu.be/e7XeKdOVq40?si=T3I4pgPoEfVytcU3 https://youtu.be/yAR6QRT3N1k?si=GQghyBwLCpijrDON https://youtu.be/hKr_TfGP7NY?si=gR2MqaLAFrqL-KBo https://youtu.be/6IHNDJ85aoQ?si=M0puk6DJpliO7ahf https://youtu.be/_v_43Wi7zjo?si=qNDVWzKcQFNO2mGh https://youtu.be/Ew3QRaKJQSA?si=xNvXvD8yFhnSMJby 3 - Understand EC2 Auto Scaling and how VM scaling works - Learn about storage and compute services (EFS/FSx, Lightsail, MGN overview) 07/10/2025 07/10/2025 https://youtu.be/bbLcPitXJSY?si=eyVnxvL9ho0LpUYy https://youtu.be/hFVYG8WqfU0?si=9Px4wmR4IRZxk15n 4 - Hands-on: + Deploy AWS Backup + Create backup plan + Test restore \u0026amp; cleanup + Clean up backup 08/10/2025 08/10/2025 https://000013.awsstudygroup.com 5 - Hands-on: + Create an S3 bucket for Storage Gateway + Create EC2 for Storage Gateway + Create Storage Gateway + File Share + Clean up Storage Gateway 09/10/2025 09/10/2025 https://000024.awsstudygroup.com 6 - Hands-on: + Create bucket, upload data + Enable static website hosting + Configure public access block + Configure CloudFront and test website + Clean up website + CloudFront + bucket 10/10/2025 10/10/2025 https://000057.awsstudygroup.com Week 5 Achievements: Summary:\nThis week I learned how EC2 operates, different instance storage types, Auto Scaling, and backup mechanisms. I also practiced Storage Gateway and deployed an S3 static website. Theory learned:\nEC2 architecture, AMI, key pair EBS vs Instance Store User Data / Metadata EC2 Auto Scaling Storage Gateway and fundamentals of AWS Backup Hands-on labs:\nCreate backup plan + test restore Create Storage Gateway + file share Deploy static website using S3 + CloudFront "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.5-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Preparation Before creating AWS resources, you need to download sample data to test the system.\nStep 1: Download Sample Data Access ARC Sample Data Download the data to your computer Extract the file, which will create a folder named DATA Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Prepare AWS Resources Step 2: Create S3 Bucket S3 Bucket is used to store uploaded PDF documents.\nSearch for S3 in AWS Console Click Create bucket Configure bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Keep other settings as default Click Create bucket 💡 Tip: To get your AWS Account ID, run:\naws sts get-caller-identity --query Account --output text Or create using CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Step 3: Create DynamoDB Table DynamoDB Table is used to store document metadata.\nSearch for DynamoDB in AWS Console Click Create table Configure table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Or create using CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Step 4: Create SQS Queue SQS Queue is used for IDP pipeline to process documents.\nSearch for SQS in AWS Console Click Create queue Configure queue: Type: Standard Name: arc-document-queue Keep other settings as default Click Create queue Or create using CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Step 5: Verify Resources Check that all resources have been created:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Step 6: Upload Data to S3 Upload PDF files from the DATA folder downloaded in Step 1:\n# Upload all files from DATA folder aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Or upload individual file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Before proceeding, make sure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with sufficient permissions Bedrock models approved (Claude + Cohere) S3 Bucket created DynamoDB Table created SQS Queue created Sample documents uploaded to S3 "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-POWERED ACADEMIC RESEARCH CHATBOT ON AWS Overview In this workshop, we will build ARC (Academic Research Chatbot) - an intelligent chatbot system running on the AWS Serverless platform. This solution leverages Generative AI and RAG (Retrieval-Augmented Generation) to support academic research, document queries, and answer questions flexibly.\nInstead of answering questions based on fixed scripts (rule-based), the system uses the Claude 3.5 Sonnet model to understand natural language, query data from the vector database, and respond to users accurately.\nWorkshop Objectives After completing this workshop, you will:\nUnderstand RAG architecture and how to apply it in practice Deploy a complete chatbot system on AWS Use Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Build an IDP pipeline with Amazon Textract Implement vector search with Qdrant Deploy infrastructure with Terraform Integrate authentication with Amazon Cognito Content Introduction Preparation Activate Bedrock Models Configure AWS CLI Data Preparation Deploy Infrastructure Setup Backend API Setup IDP Pipeline Setup Frontend Using Chatbot Using Admin Dashboard Clean up Resources "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Company Limited from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI participated in hands-on training and specialized workshops on AWS Cloud, through which I strengthened my skills in working with cloud services, technology analysis, technical reporting, and communication in a professional environment.\nRegarding work ethic, I consistently strived to complete assigned tasks, complied with company policies, and actively communicated with colleagues to improve work efficiency.\nTo objectively reflect on my internship experience, I provide the following self-evaluation:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, tool proficiency, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Self-learning, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring work quality ☐ ✅ ☐ 5 Discipline Compliance with schedules, regulations, and work processes ✅ ☐ ☐ 6 Willingness to improve Openness to feedback and continuous self-development ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and contributing to team activities ✅ ☐ ☐ 9 Professional conduct Respectful behavior toward colleagues, partners, and the workplace environment ✅ ☐ ☐ 10 Problem-solving skills Identifying issues, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to projects/organization Work effectiveness, improvement initiatives, and recognition from the team ☐ ✅ ☐ 12 Overall performance General assessment of the entire internship period ✅ ☐ ☐ Strengths Quick learner with the ability to acquire new knowledge efficiently. Strong adherence to work principles and guidelines. Responsible in completing assigned tasks. Friendly, cooperative, and supportive in team environments. Areas for Improvement Communication confidence: Need to be more proactive and confident in communication. Deep problem exploration: Improve the ability to analyze issues deeply instead of focusing only on surface-level solutions. Development Plan Create a detailed study and practice schedule to improve learning efficiency. Actively participate in discussions to enhance communication confidence and presentation skills. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn the overview of AWS storage services (S3, Glacier, Backup, Storage Gateway, Snow Family). Understand how S3 works: access point, storage class, CORS, static website hosting. Practice the entire workflow with S3, Backup, Storage Gateway, and File Systems. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the overview of AWS storage services: S3, EBS, Backup, Storage Gateway, Snow Family - Study Access Point, Storage Class, and data access models - Understand S3 static website, CORS, Object key, Glacier 13/10/2025 13/10/2025 https://youtu.be/hsCfP0IxoaM?si=O3vMWs7Trr1fugJD https://youtu.be/_yunukwcAwc?si=ZhkTKr-_OkyUNImI https://youtu.be/mPBjB6Ltl_Q?si=qs6j0n7AeD2Mxwbz https://youtu.be/YXn8Q_Hpsu4?si=XojTnkR_LLC1KwEv 3 Hands-on: + Create S3 bucket + Deploy backup infrastructure + Create backup plan and set notification + Test restore \u0026amp; clean up backup resources 14/10/2025 14/10/2025 https://000013.awsstudygroup.com 4 - Learn VMware Workstation - Hands-on: + Export VM from on-prem + Upload VM to AWS + Import as EC2 + Export back as AMI + Clean up import/export environment 15/10/2025 15/10/2025 https://000014.awsstudygroup.com 5 - Hands-on: + Create Storage Gateway + Create advanced File Share + Connect File Share from on-prem machine + Clean up Storage Gateway + File Shares 16/10/2025 16/10/2025 https://000024.awsstudygroup.com 6 - Hands-on (lab25): + Create FSx file system (SSD/HDD, Multi-AZ) + Create \u0026amp; configure file shares + Test \u0026amp; monitor performance + Manage user sessions + quotas - Hands-on (lab57): + Create bucket, upload data, enable static website + Configure public access + object permissions + Create \u0026amp; configure CloudFront distribution + Enable versioning \u0026amp; object replication - Clean up environment (lab25), bucket, CloudFront, replication 17/10/2025 17/10/2025 https://000025.awsstudygroup.com https://000057.awsstudygroup.com Week 6 Achievements: Summary:\nThis week I learned the AWS storage ecosystem including S3, Glacier, Backup, Storage Gateway, and file systems. I focused heavily on hands-on labs to understand data management, backup–restore, and AWS storage mechanisms. Theory learned:\nS3 Storage Class, Access Point, CORS Glacier, lifecycle, backup concepts Storage Gateway and file system architecture VM import/export Hands-on labs:\nBackup \u0026amp; restore Import on-prem VM into AWS Create Multi-AZ file system Create static website, CloudFront, versioning, replication Practice Storage Gateway – create file share, connect, test data transfer between on-prem and AWS "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.6-infrastructure/","title":"Deploy Infrastructure","tags":[],"description":"","content":"In this section, we will clone the repository and deploy the entire AWS infrastructure for the ARC Chatbot system.\nStep 1: Clone Repository Clone the repository from GitHub:\ngit clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Step 2: Build Dashboard Before deploying the application, we need to build the frontend dashboard.\nNavigate to the frontend folder cd frontend Install dependencies Run the following command to install the necessary libraries:\nnpm install Build Dashboard After installation is complete, run the build command:\nnpm run build After the process is complete, a dist folder will be created. Verify the index.html file and the assets folder:\nls dist/ # index.html assets/ Return to the project root directory cd .. Step 3: Deploy Terraform Application Deploy the Terraform application. The process will take approximately 20-30 minutes to deploy all resources.\ncd terraform terraform init terraform apply --auto-approve ⚠️ Note: If you encounter an error at this step, make sure Docker is running on your computer.\n💡 Info: Replace \u0026lt;account_id\u0026gt; with your actual AWS Account ID.\nStep 4: Verify Deployment After completing all the steps above, your environment has been successfully deployed.\nYou can verify the deployment by checking:\nAWS Console: Check the resources that have been created (EC2, S3, Cognito, DynamoDB, etc.) Terraform State: Run terraform state list to see the list of resources S3 Buckets: Buckets for documents and frontend have been created EC2 Instance: Instance for backend has been launched Check Outputs terraform output Important outputs:\nOutput Description api_endpoint Backend API URL cognito_user_pool_id Cognito User Pool ID cognito_client_id Cognito App Client ID s3_bucket_name S3 bucket for documents cloudfront_url Frontend URL Next Steps Now you can proceed to:\nSet up Backend Set up IDP Pipeline Set up Frontend "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment at AWS through the FCJ program is highly professional and well-organized. The learning and working space is structured effectively, enabling me to focus and absorb knowledge efficiently. FCJ regularly organizes workshops, which help me expand my understanding and stay updated with the latest technologies available on AWS.\n2. Support from Mentor / Team Admin\nMy mentor provided detailed guidance, explained clearly whenever I did not fully understand, and consistently encouraged me to ask questions. I truly appreciate that the mentor let me try and solve problems on my own instead of giving me the answer immediately, which helped me become more proactive and improve quickly. The admin team was also very supportive, preparing all necessary documents and procedures to ensure a smooth working process. Whenever I had questions, both the mentor and admin team responded quickly and provided thorough explanations, motivating me throughout the internship.\n3. Relevance of Work to My Academic Major\nDuring the internship, I learned many important skills—from technical knowledge such as working with AWS services to soft skills like teamwork, presentation, and report writing. The sharing sessions from experts and FCJ members gave me new perspectives and valuable insights for shaping my future career development.\n4. Learning \u0026amp; Skill Development Opportunities\nThroughout the internship, I gained a wide range of new skills, including the use of project management tools, effective teamwork, and professional communication in a corporate setting. My mentor also shared a lot of practical experience that helped me build clearer career directions, especially in the DevOps and cloud domain.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously, yet maintains a friendly and enjoyable atmosphere. When urgent projects arise, everyone collaborates and supports one another regardless of their position or role. This helped me feel like a true part of the team, even though I was only an intern.\n6. Internship Policies / Benefits\nThe company offers an internship allowance and provides flexible working hours when necessary. Additionally, the opportunity to participate in internal training sessions, workshops, and knowledge-sharing activities is a major advantage that supports continuous learning.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with the opportunity to work directly with DevOps and AWS tasks. Being able to deploy infrastructure, configure services, and troubleshoot real issues helped me improve quickly. I also learned a great deal about DevSecOps practices, including IAM management, CI/CD pipelines, and system operations.\nWhat do you think the company should improve for future interns?\nI think the program could include additional advanced DevOps workshops, such as building complete CI/CD pipelines, running incident simulation exercises, or conducting mini-projects in teams. This would help interns gain a deeper understanding of real-world system operations.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes, definitely! The program provides highly practical knowledge in DevOps and AWS, with hands-on experience rather than just theory. The mentors are supportive, the environment is professional yet friendly, and it is especially suitable for students pursuing a Cloud/DevOps career path.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest adding more group practical exercises followed by a final consolidated project. This would help interns strengthen teamwork and apply knowledge in a more integrated manner.\nWould you like to continue this program in the future?\nIf given the opportunity, I would love to continue the program. I hope to take part in more advanced topics such as building complete DevOps systems, multi-environment CI/CD pipelines, and AWS cost optimization.\n"},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn the complete IAM system: user, group, role, policy, permission boundary. Understand AWS authentication \u0026amp; authorization mechanisms, how to write JSON policies, and how policy evaluation works. Get familiar with AWS Organizations, Organizational Units (OU), and Service Control Policies (SCP). Practice IAM + Organization labs to understand how large-scale account management works. Tasks to be carried out this week: Day Task Start Date End Date Reference 2 - Learn the IAM fundamentals: user, group, role, policy - Understand policy evaluation: explicit deny, implicit deny, allow 20/10/2025 20/10/2025 https://youtu.be/tsobAlSg19g?si=9f3mlIWPtrCcNuKg https://youtu.be/N_vlJGAqZxo?si=e8oiWCObco95CoKh 3 - Hands-on: + Create user/group/role + Attach inline \u0026amp; managed policies + Test S3/EC2 access under different policies - Learn permission boundaries and session policies 21/10/2025 21/10/2025 https://000028.awsstudygroup.com 4 - Study AWS Organizations: OU structure, multi-account setup - Understand SCP concepts, deny list vs allow list 22/10/2025 22/10/2025 https://youtu.be/5oQY8Rogz9Y?si=h8DlUb8ZLI4HbbvM https://youtu.be/NW1xrMkNMjU?si=dhT0T3y2JYVK8QwT 5 - Hands-on: + Create Organization + OU + Apply SCP deny EC2 / deny S3 + Verify SCP effectiveness combined with IAM policies + Rearrange OU, remove SCP 23/10/2025 23/10/2025 https://000030.awsstudygroup.com https://000044.awsstudygroup.com/ 6 - Team activity: + Discuss workshop ideas + Plan execution + Divide tasks for workshop 24/10/2025 24/10/2025 Week 7 Achievements: Summary:\nThis week I learned the foundation of AWS access management, including IAM and Organizations. I now understand how policy evaluation works, how to write JSON policies, and how SCPs apply across multi-account environments. Theory learned:\nConcepts of User – Group – Role – Policy and how evaluation works Inline policy, managed policy, permission boundary AWS Organizations structure, OU SCP concepts and differences compared to IAM policies Landing Zone \u0026amp; Control Tower overview Hands-on labs:\nCreate user/group/role and test different access levels Write JSON policies and test allow/deny behaviors Create Organization, OU, and apply SCP Validate SCP + IAM policy combinations in practice "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.7-backend/","title":"Set up Backend API","tags":[],"description":"","content":"Set up Backend API In this section, you will configure the Backend API (FastAPI) and Qdrant vector database on EC2.\nBackend Architecture Internet → ALB (:80) → EC2 Private Subnet ├── FastAPI Container (:8000) ├── Qdrant Container (:6333) └── SQS Worker (background) 💡 Note: EC2 is located in a Private Subnet with no Public IP. Access via SSM Session Manager.\nStep 1: Access EC2 via Session Manager The EC2 instance was created in a Private Subnet with no Public IP. Use AWS Systems Manager Session Manager to access it.\nMethod 1: AWS Console Open AWS Console → EC2 → Instances Select instance arc-dev-app-server Click Connect → Session Manager → Connect Method 2: AWS CLI # Get Instance ID from Terraform output INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect via SSM aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 ⚠️ Requirement: Install Session Manager Plugin\nStep 2: Check Running Services EC2 has been automatically set up via user_data script when Terraform created the instance. Verify the services:\n# Switch to ec2-user sudo su - ec2-user # Check Docker containers docker ps You should see 2 containers running:\napp-fastapi-1 - FastAPI server (port 8000) app-qdrant-1 - Qdrant vector database (port 6333) # Check Qdrant curl http://localhost:6333/collections # Check FastAPI curl http://localhost:8000/health Step 3: Deploy Backend Code Backend code will be deployed via CI/CD Pipeline (CodePipeline → CodeBuild → CodeDeploy). However, for quick testing, you can deploy manually:\ncd /home/ec2-user # Clone repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project/backend # Stop old containers cd /home/ec2-user/app docker-compose down # Copy backend code cp -r /home/ec2-user/ARC-project/backend/* /home/ec2-user/app/ # Start with new code docker-compose up -d --build Step 4: Configure Environment Variables Create a .env file with values from Terraform outputs:\ncd /home/ec2-user/app # Get values from Terraform outputs (run on local machine) # terraform -chdir=terraform output cat \u0026gt; .env \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # AWS Configuration AWS_REGION=ap-southeast-1 # S3 S3_BUCKET_NAME=arc-documents-\u0026lt;account-id\u0026gt; # DynamoDB DYNAMODB_TABLE_NAME=arc-dev-documents # SQS SQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account-id\u0026gt;/arc-dev-document-queue # Qdrant (local container) QDRANT_HOST=qdrant QDRANT_PORT=6333 # Cognito COGNITO_USER_POOL_ID=ap-southeast-1_xxxxx COGNITO_CLIENT_ID=xxxxx # Bedrock BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0 EOF 💡 Tip: Replace \u0026lt;account-id\u0026gt; and xxxxx values with actual outputs from Terraform.\nStep 5: Restart Services # Restart to load new .env docker-compose down docker-compose up -d # Check logs docker-compose logs -f fastapi Step 6: Verify via ALB Backend is exposed via Application Load Balancer. Verify from your local machine:\n# Get ALB DNS from Terraform output ALB_DNS=$(terraform -chdir=terraform output -raw alb_dns_name) # Test health endpoint curl http://$ALB_DNS/health # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Step 7: Check Qdrant Collection # On EC2 curl http://localhost:6333/collections # Create collection for documents (if not exists) curl -X PUT http://localhost:6333/collections/documents \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;vectors\u0026#34;: { \u0026#34;size\u0026#34;: 1024, \u0026#34;distance\u0026#34;: \u0026#34;Cosine\u0026#34; } }\u0026#39; 💡 Note: Vector size 1024 corresponds to Amazon Titan Embeddings v2.\nChecklist Successfully accessed EC2 via Session Manager Docker containers running (fastapi, qdrant) .env file configured Health check via ALB successful Qdrant collection created Troubleshooting Unable to connect to Session Manager # Check SSM Agent on EC2 sudo systemctl status amazon-ssm-agent # Verify IAM Role has AmazonSSMManagedInstanceCore policy Container fails to start # View logs docker-compose logs # Check disk space df -h ALB health check fails # Verify Security Group allows port 8000 from ALB # Verify FastAPI is listening on 0.0.0.0:8000 docker-compose logs fastapi "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn about database systems on AWS: RDS, Aurora, Redshift, ElastiCache. Practice building a database subnet group, test connectivity, backup \u0026amp; restore. Learn data analytics services such as Kinesis, Glue, Athena, QuickSight. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Learn about Databases: RDS, Aurora, Redshift, ElastiCache - Learn about Multi-AZ architecture, read replicas, backup/restore 27/10/2025 27/10/2025 https://youtu.be/OOD2RwWuLRw?si=9JsOs0PNfO1TdAUl https://youtu.be/qbrobQZrokY?si=ePJjzYXWg3qE_Ca6 https://youtu.be/UvdiRW34aNI?si=8g3FwgsJ3VLT-_nf 3 - Practice: + Create VPC + SG for EC2 + RDS + Create DB subnet group + Deploy EC2 + Create RDS instance + Backup \u0026amp; Restore 28/10/2025 28/10/2025 https://000005.awsstudygroup.com 4 - Practice: + Connect to MSSQL/Oracle + Schema Conversion + Create DMS Task + Inspect logs, troubleshoot 29/10/2025 29/10/2025 https://000043.awsstudygroup.com 5 - Learn about Data Analytics (Kinesis, Glue, Athena, QuickSight) - Practice: + Create DynamoDB table + Enable autoscaling + CRUD test + Create Global Table and clean up resources 30/10/2025 30/10/2025 https://000039.awsstudygroup.com 6 - Practice (lab35): + Create S3 bucket + Create Kinesis Firehose ingestion + Glue crawler + Query data with Athena + Create QuickSight dashboard - Practice (lab40): + Check cost allocation + Tagging resources + Additional queries \u0026amp; resource cleanup 31/10/2025 31/10/2025 https://000035.awsstudygroup.com https://000040.awsstudygroup.com Week 8 Achievements: Overview:\nThis week, I focused on AWS database and data analytics services, including RDS, Aurora, DynamoDB, DMS, Kinesis, Glue, Athena, and QuickSight. I gained a solid understanding of database architecture, connectivity, backup/restore, autoscaling, as well as the end-to-end data analytics pipeline from ingestion -\u0026gt; ETL -\u0026gt; query -\u0026gt; visualization. Theory Learned:\nConcepts of RDS, Aurora architecture, Multi-AZ, read replicas Backup, snapshot, parameter group, option group DynamoDB: partition key, sort key, throughput, autoscaling, DAX Overview of Data Analytics: Kinesis Firehose, Glue crawler, Athena, QuickSight Database Migration concepts: schema conversion, DMS task Lab Practice:\nCreated VPC + security groups for EC2/RDS Created DB subnet group, deployed EC2 and RDS MySQL Performed Backup \u0026amp; Restore Connected to MSSQL/Oracle, practiced Schema Conversion \u0026amp; created DMS task Created DynamoDB table, enabled autoscaling, CRUD test, created Global Table \u0026amp; cleanup Built analytics pipeline: Kinesis Firehose -\u0026gt; S3, Glue crawler, Athena queries, and QuickSight dashboard Performed additional tasks: tagging, cost allocation, and resource cleanup "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.8-idp-pipeline/","title":"Set up IDP Pipeline","tags":[],"description":"","content":"Set up IDP Pipeline In this section, you will setup SQS Worker to process documents through IDP (Intelligent Document Processing) pipeline.\nIDP Flow Upload → S3 → DynamoDB (UPLOADED) → SQS ↓ EC2 Worker ↓ PyPDF2 (digital) / Textract (scanned) ↓ Chunk Text (1000 tokens) ↓ Cohere Embed Multilingual v3 (Bedrock) ↓ Qdrant (store vectors) ↓ DynamoDB (EMBEDDING_DONE) 💡 Note: Worker uses PyPDF2 for digital PDF (text-based) and Textract for scanned PDF (image-based).\nDocument States Status Description UPLOADED File uploaded, waiting for processing IDP_RUNNING Worker is processing TEXTRACT_DONE OCR completed (scanned PDF only) EMBEDDING_DONE Completed, ready to use FAILED Error occurred Step 1: Access EC2 via Session Manager # Get Instance ID INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 After connecting:\nsudo su - ec2-user cd /home/ec2-user/backend 💡 Note: EC2 has 2 folders:\napp/ - Boilerplate from user_data script backend/ - Actual code deployed via CI/CD (contains run_worker.py) Step 2: Check Worker Code Worker code is in backend/run_worker.py. Verify the file exists:\nls -la # Must have: run_worker.py, app/, requirements.txt Step 3: Configure Environment Ensure .env file has all variables (in backend/ folder):\ncd /home/ec2-user/backend cat .env Important variables for IDP:\nSQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue S3_BUCKET=arc-documents-\u0026lt;account\u0026gt; QDRANT_HOST=localhost QDRANT_PORT=6333 AWS_REGION=ap-southeast-1 Step 4: Start Worker Option A: Run directly (for debugging) # Activate virtual environment (if available) source venv/bin/activate # Run worker python run_worker.py Worker will display:\n============================================================ IDP Pipeline - SQS Worker ============================================================ Queue URL: https://sqs.ap-southeast-1.amazonaws.com/xxx/arc-dev-document-queue Bucket: arc-documents-xxx Region: ap-southeast-1 Qdrant: localhost:6333 ------------------------------------------------------------ Processing indefinitely (Ctrl+C to stop)... Option B: Run in background with nohup nohup python run_worker.py \u0026gt; worker.log 2\u0026gt;\u0026amp;1 \u0026amp; # Check process ps aux | grep run_worker # View logs tail -f worker.log Option C: Run in Docker (recommended) # Add worker to docker-compose.yml docker-compose up -d worker Step 5: Test IDP Pipeline 5.1 Upload test file to S3 # From local machine aws s3 cp test-sample.pdf s3://arc-documents-\u0026lt;account\u0026gt;/uploads/test-001.pdf 5.2 Create record in DynamoDB aws dynamodb put-item \\ --table-name arc-dev-documents \\ --item \u0026#39;{ \u0026#34;doc_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-001\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;METADATA\u0026#34;}, \u0026#34;filename\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-sample.pdf\u0026#34;}, \u0026#34;s3_key\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UPLOADED\u0026#34;}, \u0026#34;uploaded_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;} }\u0026#39; 5.3 Send message to SQS aws sqs send-message \\ --queue-url https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue \\ --message-body \u0026#39;{ \u0026#34;doc_id\u0026#34;: \u0026#34;test-001\u0026#34;, \u0026#34;s3_key\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;test-sample.pdf\u0026#34; }\u0026#39; Step 6: Monitor Processing View worker logs:\n# If running directly # Logs displayed on terminal # If running in background tail -f worker.log Successful logs will look like:\n2024-01-15 10:30:00 - INFO - Received message for doc_id: test-001 2024-01-15 10:30:01 - INFO - Downloading from S3: uploads/test-001.pdf 2024-01-15 10:30:02 - INFO - Extracting text with PyPDF2... 2024-01-15 10:30:03 - INFO - Created 8 chunks from document 2024-01-15 10:30:05 - INFO - Generating embeddings with Cohere... 2024-01-15 10:30:10 - INFO - Stored 8 vectors for test-001 2024-01-15 10:30:10 - INFO - Updated status: EMBEDDING_DONE 2024-01-15 10:30:10 - INFO - Document test-001 processed successfully Step 7: Verify Processing 7.1 Check DynamoDB aws dynamodb get-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --query \u0026#39;Item.{status:status.S,chunks:chunk_count.N}\u0026#39; Expected output:\n{ \u0026#34;status\u0026#34;: \u0026#34;EMBEDDING_DONE\u0026#34;, \u0026#34;chunks\u0026#34;: \u0026#34;8\u0026#34; } 7.2 Check Qdrant # On EC2 curl -s http://localhost:6333/collections/arc_documents/points/count | jq Expected output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;count\u0026#34;: 8 } } Error Handling Issue Cause Solution Worker not receiving messages SQS URL incorrect Check .env Bedrock timeout Rate limit Increase retry delay Qdrant connection refused Container not started docker-compose up -d qdrant FAILED status Check error_message in DynamoDB Fix and retry Retry Failed Document # Update status back to UPLOADED to retry aws dynamodb update-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --update-expression \u0026#34;SET #s = :s\u0026#34; \\ --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;:\u0026#34;status\u0026#34;}\u0026#39; \\ --expression-attribute-values \u0026#39;{\u0026#34;:s\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;UPLOADED\u0026#34;}}\u0026#39; # Send message to SQS again aws sqs send-message \\ --queue-url $SQS_QUEUE_URL \\ --message-body \u0026#39;{\u0026#34;doc_id\u0026#34;:\u0026#34;test-001\u0026#34;,\u0026#34;s3_key\u0026#34;:\u0026#34;uploads/test-001.pdf\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;test-sample.pdf\u0026#34;}\u0026#39; Checklist Access EC2 via Session Manager Worker code is available Environment variables configured Worker is running Test document uploaded to S3 SQS message sent Worker processed document (logs) Status = EMBEDDING_DONE in DynamoDB Vectors stored in Qdrant "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn the overall Serverless architecture and its core components: API Gateway, Lambda, DynamoDB. Design the full application architecture: backend, frontend, authentication, security, and integrations. Explore the security and edge layer: CloudFront, Route 53, WAF. Learn the authentication system with Amazon Cognito and how tokens are issued for APIs. Analyze how CI/CD is implemented using CodePipeline, CodeBuild, GitLab, and IaC (CloudFormation). Learn AWS observability and monitoring systems: CloudWatch, X-Ray, SNS. Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Learn Serverless architecture - API Gateway, Lambda, DynamoDB \u0026amp; integration mechanisms 03/11/2025 03/11/2025 3 - Design full architecture (BE + FE + Auth + Edge) - Draw architecture diagram \u0026amp; request flow 04/11/2025 04/11/2025 4 - Learn CloudFront, Route 53, WAF - Design security layer in front of API Gateway 05/11/2025 05/11/2025 5 - Learn Cognito (User Pool, Token) - Analyze how API Gateway validates JWT tokens 06/11/2025 06/11/2025 6 - Learn CI/CD: CloudFormation, CodePipeline, CodeBuild - Observability system: CloudWatch, X-Ray, SNS 07/11/2025 07/11/2025 Week 9 Achievements: Overview:\nThis week, I focused on understanding and designing the Serverless architecture for the application. I gained a solid understanding of how API Gateway – Lambda – DynamoDB work together, and explored the security layer using CloudFront/WAF as well as authentication via Cognito. I also learned the CI/CD workflow and logging/monitoring mechanisms to prepare for the coding phases in the upcoming weeks. Theory Learned:\nServerless architecture, pay-per-use model, and autoscaling principles. API Gateway REST API, Lambda integration, and DynamoDB table workflow. CloudFront + WAF + Route 53 for API protection. Cognito User Pool \u0026amp; Tokens (ID/Access), JWT flow through API Gateway authorizer. CI/CD with CodePipeline + CodeBuild + CloudFormation. CloudWatch logs/metrics, SNS alerts, tracing using X-Ray. Practice / Deliverables:\nFull system architecture diagram (BE – FE – Auth – Edge). Request flow from client -\u0026gt; CloudFront -\u0026gt; API Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB. Security diagram: CDN, DNS, WAF, throttling \u0026amp; protection at API Gateway. Preliminary CI/CD pipeline design using CloudFormation \u0026amp; CodePipeline. Authentication flow design: Cognito -\u0026gt; API Gateway JWT Authorizer. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.9-frontend/","title":"Setup Frontend","tags":[],"description":"","content":"Setup Frontend Configure and deploy Frontend React application with AWS Amplify.\nStep 1: Get Terraform Outputs cd terraform terraform output Note: cognito_user_pool_id, cognito_client_id, alb_dns_name\nStep 2: Configure Environment cd ARC-project cp .env.example .env Edit .env:\nVITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_POOL_ID=ap-southeast-1_xxxxxxx VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxx VITE_API_URL=http://arc-chatbot-dev-alb-xxxxx.ap-southeast-1.elb.amazonaws.com Step 3: Install \u0026amp; Test Local npm install npm run dev Open http://localhost:5173\nStep 4: Build \u0026amp; Deploy npm run build Push code to GitHub, Amplify will deploy automatically:\ngit add . git commit -m \u0026#34;Update frontend config\u0026#34; git push origin main 💡 Amplify app was created via Terraform and connected with GitHub.\nStep 5: Update Cognito Callback URLs After getting Amplify URL:\naws cognito-idp update-user-pool-client \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --client-id xxxxxxxxxx \\ --callback-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; \\ --logout-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; Step 6: Create Test Users # Admin user aws cognito-idp admin-create-user \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --user-attributes Name=email,Value=admin@example.com \\ --temporary-password \u0026#34;TempPass123!\u0026#34; aws cognito-idp admin-add-user-to-group \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --group-name admin Error Handling Error Solution CORS error Check FastAPI CORS config \u0026ldquo;User pool does not exist\u0026rdquo; Check VITE_COGNITO_POOL_ID Build failed Check Amplify environment variables Checklist .env configured Local dev server running Amplify deploy successful Cognito callback URLs updated Login/Register working "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Complete 100% of the Backend CRUD using .NET running on Aspire AppHost. Build and test the DynamoDB data model on the local environment (Docker). Integrate DynamoDB Local with NoSQL Workbench to validate the data model. Collaborate with the FE team to complete the basic UI and connect to local APIs. Prepare a solid foundation before moving into Week 11 (migrating code to Lambda + API Gateway). Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Verify \u0026amp; reinstall BE environment: .NET SDK, Docker Desktop, NoSQL Workbench - Understand system architecture diagram and backend processing flow 10/11/2025 10/11/2025 3 - Set up DynamoDB Local using Docker - Connect \u0026amp; visualize via NoSQL Workbench, manually test CRUD operations 11/11/2025 11/11/2025 4 - Implement DAL/Repository using .NET + AWSSDK.DynamoDBv2 - Integrate DI into Aspire AppHost - Implement business logic in BLL 12/11/2025 12/11/2025 5 - Build CRUD API Controllers - Perform unit tests \u0026amp; integration tests in the local environment 13/11/2025 13/11/2025 6 - Collaborate with the FE team - Build basic UI (List/Create) - FE connects to Local API and performs end-to-end testing 14/11/2025 14/11/2025 Week 10 Achievements: Overview:\nThis week, I completed the entire backend running locally using Aspire AppHost, built the DynamoDB data model, tested CRUD operations, and integrated with the frontend. By ensuring a complete and stable local architecture and codebase, I established a strong foundation for transitioning to Lambda/API Gateway in Week 11. Theoretical knowledge acquired:\nDynamoDB data modeling based on Single-Table Design, Access Patterns, PK/SK. Knowledge about DynamoDB Local, NoSQL Workbench, Docker setup. Using AWSSDK.DynamoDBv2 in .NET and integrating Dependency Injection. Backend structure in ASP.NET (Controller -\u0026gt; BLL -\u0026gt; Repository). How FE calls local APIs, handles responses, and renders UI. Practical work / Deliverables:\nSet up DynamoDB Local using Docker and managed schema via NoSQL Workbench. Built Repository + BLL + Controllers using .NET 8 / Aspire AppHost. Completed 5–6 CRUD APIs (POST/GET/PUT/DELETE/List). Wrote unit tests for BLL and integration tests for API. FE team built List + Create UI and successfully tested with local API. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.10-using-chatbot/","title":"Using Chatbot","tags":[],"description":"","content":"Guide to using ARC Chatbot to search for information from research documents.\nAccess Local: http://localhost:5173 Production: Amplify URL from previous step Step 1: Login Enter email and password Click Login Redirect to Chat page Step 2: Chat Interface After logging in, you will see:\nSidebar with Chat, History menus Header with user info and dark mode toggle Chat area with welcome message Step 3: Ask Questions Enter a question in the input box and press Enter or click Send.\nGood Questions Type Example Definition \u0026ldquo;What is a stack data structure?\u0026rdquo; Comparison \u0026ldquo;Compare stack and queue\u0026rdquo; Explanation \u0026ldquo;Explain binary search algorithm\u0026rdquo; Avoid ❌ Too general: \u0026ldquo;Tell me about programming\u0026rdquo; ❌ Outside documents: \u0026ldquo;What\u0026rsquo;s the weather today?\u0026rdquo; Step 4: Citations Each answer has citations showing document sources:\n📚 Sources: [1] data-structures.pdf - Page 12 - Score: 85% [2] algorithms.pdf - Page 45 - Score: 72% Click on citation to view document details.\nField Description [1], [2] Citation number Filename PDF filename Page Page number Score Relevance (%) Step 5: Conversation History Click History in sidebar View list of previous conversations Click conversation to reload it Click trash icon to delete Step 6: New Chat Click New Chat in sidebar to start a new conversation.\nFeatures Feature Description Streaming Response displays in parts Markdown Supports code blocks, lists, headers Dark Mode Toggle in header History Save and reload conversations Checklist Successfully logged in Sent query and received response Citations displayed correctly Click citation to view document History working New Chat working "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete the remaining FE interface together with the Frontend team. Coordinate end-to-end testing between FE and BE. Learn the overall deployment workflow of the system (API, FE, database, infrastructure) to gain general understanding, even if not directly responsible. Prepare the necessary notes for Week 12 (documentation \u0026amp; final summary). Tasks to be carried out this week: Day Task Start Date End Date Reference Materials 2 - Work with the FE team to complete the remaining UI components - Review API contract alignment between FE and BE 17/11/2025 17/11/2025 3 - FE integrates full CRUD API flows - Fix schema, payload, and status code mismatches during FE testing 18/11/2025 18/11/2025 4 - Perform full end-to-end testing: List -\u0026gt; Create -\u0026gt; Update -\u0026gt; Delete from FE to BE - Update response models/validations to match FE expectations 19/11/2025 19/11/2025 \u0026lt; \u0026gt; 5 - Learn the team’s deployment workflow (CI/CD, API Gateway, Lambda, S3 + CloudFront) - Take notes for Week 12 documentation 20/11/2025 20/11/2025 6 - Summarize FE and BE issues during the week - Review the entire BE codebase to prepare for deployment environment (even though not the one deploying) 21/11/2025 21/11/2025 Week 11 Achievements: Overview:\nThis week, I mainly collaborated with the FE team to finalize UI components and complete API integration. Since the backend was completed in Week 10, I helped fix issues, standardize API contracts, and conduct full end-to-end testing.\nAdditionally, I learned the team’s deployment workflow (Lambda, API Gateway, S3/CloudFront, CI/CD) to prepare documentation for Week 12. Theoretical knowledge acquired:\nHow FE calls CRUD APIs and debugs requests. API Contract: input/output schemas, error formats, status codes. Deployment workflow: .NET -\u0026gt; Lambda, API Gateway routing, FE build -\u0026gt; S3/CloudFront. Overview of CI/CD pipeline and how to prepare code for deployment (configs, logging). Practical work / Deliverables:\nCollaborated with FE to complete List/Create/Update/Delete screens. Adjusted backend according to FE requirements (schema, validation, status codes). Conducted full end-to-end testing between FE ↔ BE on local environment. Documented deployment workflow for use in Week 12. Compiled FE and BE issues and updated the team backlog. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.11-admin-dashboard/","title":"Using Admin Dashboard","tags":[],"description":"","content":"Using Admin Dashboard Guide to using Admin Dashboard to manage documents.\nAccess URL: http://localhost:5173/admin (or Amplify URL) Requirement: Account in admin group Step 1: Login Admin Log in with admin account (created in previous step).\nStep 2: Dashboard Overview After logging in, you will see:\nUpload section (drag \u0026amp; drop) Documents table with pagination Status filter and auto-refresh Step 3: Upload Documents 3.1. Select files Drag \u0026amp; drop PDF files to upload area Or click Browse Files to select 3.2. Upload Progress Each file displays progress bar and status:\nuploading - Uploading success - Upload successful error - Upload failed Step 4: Document Status After upload, document will be processed through IDP pipeline:\nStatus Description Time UPLOADED Waiting for processing - IDP_RUNNING Processing 1-5 min EMBEDDING_DONE Ready - FAILED Error - 💡 Tip: Enable Auto-refresh (5s) to automatically update status.\nStep 5: Manage Documents Filter by Status Use Status dropdown to filter:\nAll Uploaded Processing Done Failed Pagination Documents are paginated (5 items/page). Use pagination controls at footer.\nView Document Click 👁️ icon to view document details.\nDelete Document Click 🗑️ icon to delete document.\n⚠️ Warning: Deleting document will delete from S3, DynamoDB, and Qdrant.\nStep 6: Processing History Click Processing History link to view document processing history.\nError Handling Issue Solution Upload failed Check file size (\u0026lt;50MB), format (PDF only) Document stuck in IDP_RUNNING Check worker logs on EC2 Document FAILED See error message in Processing History Checklist Logged in to admin dashboard Upload document successful Document processed (EMBEDDING_DONE) Filter/pagination working Auto-refresh working "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/1-worklog/1.12-week12/","title":"Worklog","tags":[],"description":"","content":"*# Week 12 Worklog\nWeek 12 Objectives Master the deployment process of AWS infrastructure using CDK and understand the relationships between stacks (VPC, RDS, Lambda, S3, API Gateway). Complete the backend logic for the admin module, including data processing, analytics, and the archiving mechanism from RDS → S3. Optimize the system structure, refactor backend and dashboard code toward a clean business-layer separation – easy to maintain – easy to scale. Research and implement improvements for more efficient data processing (checksum, data load limitation, optimized synchronization logic). Become proficient in managing AWS resources through CLI and Console, and ensure multiple deployments can run without conflicts. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 Refactor the deployment process and comment out unused analytics components.\nAdjust app.py to always deploy VPC, RDS, and DB Init Stack; enable DashboardStack with correct dependencies.\nComment out all Glue, archive, and analytics resources (Lambda/EventBridge + API) in dashboard_stack.py.\nReduce RDS instance size (SMALL → MICRO) and set storage = 20GB. 24/11/2025 24/11/2025 3 Begin deployment of AppStack, DashboardStack, and DBInitStack to support the admin backend.\nFix errors caused by setting max AZ = 1 when creating RDS (AWS requires minimum 2 AZ). Although 2 AZ are created, Multi-AZ mode is not enabled, only an additional subnet – so no additional cost.\nRefactor API: handle string body returned from API Gateway, update endpoint paths, improve logging.\nUpdate CDK: use correct secret for Lambda, update VPC/subnets, add HTTPS SG rule, comment out unused Bedrock endpoint.\nUpdate asset bundling (use cp -r instead of cp -au).\nReorganize the admin business flow: manage appointments, assign consultants to timeslots, and generate daily appointment summaries.\nAdmin uploads SQL file → ArchiveData stores it in S3 → future deployments will load from S3 instead of requiring re-upload.\nFuture direction: allow Lambda index.py to read schema during deployment and load data from S3 – feasible.\nLong-term direction: migrate all on-premise data to S3; Lambda will push initial data into RDS upon first deployment; new data updates will be handled by ArchiveData. 25/11/2025 25/11/2025 4 Hardcode schema in index.py and remove schema files from the project folder.\nProcess new data and save CSV for future Athena DDL usage.\nAdd dashboard overview API and UI; refactor admin database logic.\nAdd APIs for summary statistics: general overview, customers, consultants, appointments, community programs; update dashboard UI to display these metrics.\nRefactor DatabasePage: remove raw SQL execution feature, focus on schema + analytics.\nUpdate CDK + IAM roles for new S3 bucket and proper permissions.\nMove all admin dashboard business logic to a dedicated service class (code/services/admin.py).\nRename AdminStack → FrontendStack to reflect correct responsibility; update all references. 26/11/2025 26/11/2025 5 Meeting with team to design strategy for loading data from S3 to RDS whenever RDS is started.\nRequirement: avoid loading the entire dataset every time. Dynamic data (appointments, work schedules) changes frequently → cannot reload everything.\nSolution: limit the amount of data loaded. Static data (consultant info) should be fully loaded. Dynamic time-based data (appointments) should be conditionally loaded, e.g., load data for the date range relevant to admin operations.\nCurrent approach: load data within ±1 day of the current date.\nUpdate vpc_stack to import the manually created S3 bucket from CLI.\nCLI command:\naws s3 mb s3://meetassist-data-\u0026lt;account-id\u0026gt;-ap-southeast-1 --region ap-southeast-1\nClean up DashboardStack, remove outdated Glue \u0026amp; analytics code.\nUpdate APIs for customer and consultant; enhance logging for AdminManager Lambda. 27/11/2025 27/11/2025 6 Manage EventBridge rule for ArchiveData Lambda (enable/disable/check status/manual invoke).\nDisable: aws events disable-rule --name MeetAssist-ArchiveSchedule\nEnable: aws events enable-rule --name MeetAssist-ArchiveSchedule\nCheck status: aws events describe-rule --name MeetAssist-ArchiveSchedule\nInvoke: aws lambda invoke --function-name DashboardStack-ArchiveData --payload \u0026quot;{}\u0026quot; --cli-binary-format raw-in-base64-out NUL\nFix CSV UTF-8 issue when opening in Excel; index.py now supports both UTF-8 and UTF-8 BOM.\nImplement checksum logic for ArchiveData to avoid re-uploading unchanged CSV → reduce S3 PUT cost.\nOption 1: No checksum (always overwrite) – simple but noisy timestamps and higher cost.\nOption 2 (recommended): Use checksum (MD5). If unchanged → skip; if different → upload.\nCreate archive_info.json in S3 to:\n- Track archiving status\n- Store checksums\n- Record metrics, errors, last update time\n- Potential future use in the dashboard\nImplement RDS → S3 archiving mechanism using a scheduled Lambda:\n- Create ArchiveService to export CSV + upload to S3 + write metadata\n- archive_handler runs on schedule (every 5 minutes, disabled by default)\n- Automatically skip unchanged tables based on checksum\nUpdate CDK to deploy Lambda + IAM + EventBridge rule.\nImprove error handling for consultant/appointment/program workflows.\nUpdate dashboard_handler docstring to clearly describe the separation between CRUD and archiving flows. 28/11/2025 28/11/2025 Week 12 Achievements Refactored the entire backend \u0026amp; infrastructure for better optimization and maintainability. Gained a solid understanding of deploying and configuring VPC, RDS, S3, Lambda, and EventBridge using CDK. Completed all dashboard and admin APIs for analytics and management features. Optimized the import \u0026amp; archive workflows between RDS ↔ S3. Built a checksum mechanism to reduce S3 cost and improve data processing efficiency. Clearly separated business logic into service classes for easier testing and expansion. Improved logging, debugging, and API Gateway – Lambda integration structure. Completed the data migration process from on-premise to cloud and ensured correct handling on redeployments. "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/5-workshop/5.12-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources After completing the workshop, clean up AWS resources to avoid incurring charges.\n⚠️ Warning: These steps will PERMANENTLY DELETE all data and resources!\nCleanup Order Stop services on EC2 Empty S3 buckets Terraform destroy Verify cleanup Step 1: Stop Services on EC2 Connect EC2 via Session Manager:\nINSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Stop Docker containers:\nsudo su - ec2-user cd /home/ec2-user/app # Stop containers docker-compose down # Remove volumes docker volume rm app_qdrant_storage Step 2: Empty S3 Buckets S3 buckets must be empty before Terraform destroy:\n# Get bucket name from Terraform output BUCKET=$(terraform -chdir=terraform output -raw s3_bucket_name) # Empty bucket aws s3 rm s3://$BUCKET --recursive # Or force delete aws s3 rb s3://$BUCKET --force Step 3: Terraform Destroy cd terraform terraform plan -destroy terraform destroy Enter yes when prompted. This process takes about 10-15 minutes.\nStep 4: Manual Cleanup (if needed) If there are still resources not deleted:\n# CloudWatch Log Groups aws logs describe-log-groups --log-group-name-prefix /aws/arc | jq -r \u0026#39;.logGroups[].logGroupName\u0026#39; | xargs -I {} aws logs delete-log-group --log-group-name {} # EC2 Key Pair (if created manually) aws ec2 delete-key-pair --key-name arc-keypair # Amplify App (if created manually) aws amplify list-apps | jq -r \u0026#39;.apps[] | select(.name | contains(\u0026#34;arc\u0026#34;)) | .appId\u0026#39; | xargs -I {} aws amplify delete-app --app-id {} Step 5: Verify Cleanup # Check EC2 aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*arc*\u0026#34; --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; # Check S3 aws s3 ls | grep arc # Check RDS/DynamoDB aws dynamodb list-tables --query \u0026#39;TableNames[?contains(@, `arc`)]\u0026#39; # Check Lambda aws lambda list-functions --query \u0026#39;Functions[?contains(FunctionName, `arc`)].FunctionName\u0026#39; # Check ECR aws ecr describe-repositories --query \u0026#39;repositories[?contains(repositoryName, `arc`)].repositoryName\u0026#39; Expected output: All empty.\nCost Estimation Before cleanup, check estimated costs:\n# CloudWatch - Check billing dashboard for actual charges # https://console.aws.amazon.com/billing/ Checklist EC2 services stopped S3 buckets emptied Terraform destroy completed Manual cleanup verified All resources deleted Billing dashboard checked "},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khanguyense.github.io/fcj_khantse183212_Internship-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]